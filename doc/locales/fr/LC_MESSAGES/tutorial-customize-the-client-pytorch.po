# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2022 Adap GmbH
# This file is distributed under the same license as the Flower package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2023.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Flower \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-08-29 14:25+0200\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: fr\n"
"Language-Team: fr <LL@li.org>\n"
"Plural-Forms: nplurals=2; plural=(n > 1);\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.12.1\n"

#: ../../source/tutorial-customize-the-client-pytorch.ipynb:9
msgid "Customize the client"
msgstr ""

#: ../../source/tutorial-customize-the-client-pytorch.ipynb:11
msgid ""
"Welcome to the fourth part of the Flower federated learning tutorial. In "
"the previous parts of this tutorial, we introduced federated learning "
"with PyTorch and Flower (`part 1 <https://flower.dev/docs/framework"
"/tutorial-get-started-with-flower-pytorch.html>`__), we learned how "
"strategies can be used to customize the execution on both the server and "
"the clients (`part 2 <https://flower.dev/docs/framework/tutorial-use-a"
"-federated-learning-strategy-pytorch.html>`__), and we built our own "
"custom strategy from scratch (`part 3 <https://flower.dev/docs/framework"
"/tutorial-build-a-strategy-from-scratch-pytorch.html>`__)."
msgstr ""

#: ../../source/tutorial-customize-the-client-pytorch.ipynb:14
msgid ""
"In this notebook, we revisit ``NumPyClient`` and introduce a new "
"baseclass for building clients, simply named ``Client``. In previous "
"parts of this tutorial, we've based our client on ``NumPyClient``, a "
"convenience class which makes it easy to work with machine learning "
"libraries that have good NumPy interoperability. With ``Client``, we gain"
" a lot of flexibility that we didn't have before, but we'll also have to "
"do a few things the we didn't have to do before."
msgstr ""

#: ../../source/tutorial-customize-the-client-pytorch.ipynb:16
msgid ""
"`Star Flower on GitHub <https://github.com/adap/flower>`__ ‚≠êÔ∏è and join "
"the Flower community on Slack to connect, ask questions, and get help: "
"`Join Slack <https://flower.dev/join-slack>`__ üåº We'd love to hear from "
"you in the ``#introductions`` channel! And if anything is unclear, head "
"over to the ``#questions`` channel."
msgstr ""

#: ../../source/tutorial-customize-the-client-pytorch.ipynb:18
msgid ""
"Let's go deeper and see what it takes to move from ``NumPyClient`` to "
"``Client``!"
msgstr ""

#: ../../source/tutorial-customize-the-client-pytorch.ipynb:30
msgid "Step 0: Preparation"
msgstr ""

#: ../../source/tutorial-customize-the-client-pytorch.ipynb:32
msgid ""
"Before we begin with the actual code, let's make sure that we have "
"everything we need."
msgstr ""

#: ../../source/tutorial-customize-the-client-pytorch.ipynb:44
msgid "Installing dependencies"
msgstr ""

#: ../../source/tutorial-customize-the-client-pytorch.ipynb:46
msgid "First, we install the necessary packages:"
msgstr ""

#: ../../source/tutorial-customize-the-client-pytorch.ipynb:66
msgid ""
"Now that we have all dependencies installed, we can import everything we "
"need for this tutorial:"
msgstr ""

#: ../../source/tutorial-customize-the-client-pytorch.ipynb:102
msgid ""
"It is possible to switch to a runtime that has GPU acceleration enabled "
"(on Google Colab: ``Runtime > Change runtime type > Hardware acclerator: "
"GPU > Save``). Note, however, that Google Colab is not always able to "
"offer GPU acceleration. If you see an error related to GPU availability "
"in one of the following sections, consider switching back to CPU-based "
"execution by setting ``DEVICE = torch.device(\"cpu\")``. If the runtime "
"has GPU acceleration enabled, you should see the output ``Training on "
"cuda``, otherwise it'll say ``Training on cpu``."
msgstr ""

#: ../../source/tutorial-customize-the-client-pytorch.ipynb:115
msgid "Data loading"
msgstr ""

#: ../../source/tutorial-customize-the-client-pytorch.ipynb:117
msgid ""
"Let's now load the CIFAR-10 training and test set, partition them into "
"ten smaller datasets (each split into training and validation set), and "
"wrap everything in their own ``DataLoader``."
msgstr ""

#: ../../source/tutorial-customize-the-client-pytorch.ipynb:168
msgid "Model training/evaluation"
msgstr ""

#: ../../source/tutorial-customize-the-client-pytorch.ipynb:170
msgid ""
"Let's continue with the usual model definition (including "
"``set_parameters`` and ``get_parameters``), training and test functions:"
msgstr ""

#: ../../source/tutorial-customize-the-client-pytorch.ipynb:259
msgid "Step 1: Revisiting NumPyClient"
msgstr ""

#: ../../source/tutorial-customize-the-client-pytorch.ipynb:261
msgid ""
"So far, we've implemented our client by subclassing "
"``flwr.client.NumPyClient``. The three methods we implemented are "
"``get_parameters``, ``fit``, and ``evaluate``. Finally, we wrap the "
"creation of instances of this class in a function called ``client_fn``:"
msgstr ""

#: ../../source/tutorial-customize-the-client-pytorch.ipynb:309
msgid ""
"We've seen this before, there's nothing new so far. The only *tiny* "
"difference compared to the previous notebook is naming, we've changed "
"``FlowerClient`` to ``FlowerNumPyClient`` and ``client_fn`` to "
"``numpyclient_fn``. Let's run it to see the output we get:"
msgstr ""

#: ../../source/tutorial-customize-the-client-pytorch.ipynb:339
msgid ""
"This works as expected, two clients are training for three rounds of "
"federated learning."
msgstr ""

#: ../../source/tutorial-customize-the-client-pytorch.ipynb:341
msgid ""
"Let's dive a little bit deeper and discuss how Flower executes this "
"simulation. Whenever a client is selected to do some work, "
"``start_simulation`` calls the function ``numpyclient_fn`` to create an "
"instance of our ``FlowerNumPyClient`` (along with loading the model and "
"the data)."
msgstr ""

#: ../../source/tutorial-customize-the-client-pytorch.ipynb:343
msgid ""
"But here's the perhaps surprising part: Flower doesn't actually use the "
"``FlowerNumPyClient`` object directly. Instead, it wraps the object to "
"makes it look like a subclass of ``flwr.client.Client``, not "
"``flwr.client.NumPyClient``. In fact, the Flower core framework doesn't "
"know how to handle ``NumPyClient``'s, it only knows how to handle "
"``Client``'s. ``NumPyClient`` is just a convenience abstraction built on "
"top of ``Client``."
msgstr ""

#: ../../source/tutorial-customize-the-client-pytorch.ipynb:345
msgid ""
"Instead of building on top of ``NumPyClient``, we can directly build on "
"top of ``Client``."
msgstr ""

#: ../../source/tutorial-customize-the-client-pytorch.ipynb:357
msgid "Step 2: Moving from ``NumPyClient`` to ``Client``"
msgstr ""

#: ../../source/tutorial-customize-the-client-pytorch.ipynb:359
msgid ""
"Let's try to do the same thing using ``Client`` instead of "
"``NumPyClient``."
msgstr ""

#: ../../source/tutorial-customize-the-client-pytorch.ipynb:465
msgid ""
"Before we discuss the code in more detail, let's try to run it! Gotta "
"make sure our new ``Client``-based client works, right?"
msgstr ""

#: ../../source/tutorial-customize-the-client-pytorch.ipynb:490
msgid ""
"That's it, we're now using ``Client``. It probably looks similar to what "
"we've done with ``NumPyClient``. So what's the difference?"
msgstr ""

#: ../../source/tutorial-customize-the-client-pytorch.ipynb:492
msgid ""
"First of all, it's more code. But why? The difference comes from the fact"
" that ``Client`` expects us to take care of parameter serialization and "
"deserialization. For Flower to be able to send parameters over the "
"network, it eventually needs to turn these parameters into ``bytes``. "
"Turning parameters (e.g., NumPy ``ndarray``'s) into raw bytes is called "
"serialization. Turning raw bytes into something more useful (like NumPy "
"``ndarray``'s) is called deserialization. Flower needs to do both: it "
"needs to serialize parameters on the server-side and send them to the "
"client, the client needs to deserialize them to use them for local "
"training, and then serialize the updated parameters again to send them "
"back to the server, which (finally!) deserializes them again in order to "
"aggregate them with the updates received from other clients."
msgstr ""

#: ../../source/tutorial-customize-the-client-pytorch.ipynb:495
msgid ""
"The only *real* difference between Client and NumPyClient is that "
"NumPyClient takes care of serialization and deserialization for you. It "
"can do so because it expects you to return parameters as NumPy ndarray's,"
" and it knows how to handle these. This makes working with machine "
"learning libraries that have good NumPy support (most of them) a breeze."
msgstr ""

#: ../../source/tutorial-customize-the-client-pytorch.ipynb:497
msgid ""
"In terms of API, there's one major difference: all methods in Client take"
" exactly one argument (e.g., ``FitIns`` in ``Client.fit``) and return "
"exactly one value (e.g., ``FitRes`` in ``Client.fit``). The methods in "
"``NumPyClient`` on the other hand have multiple arguments (e.g., "
"``parameters`` and ``config`` in ``NumPyClient.fit``) and multiple return"
" values (e.g., ``parameters``, ``num_example``, and ``metrics`` in "
"``NumPyClient.fit``) if there are multiple things to handle. These "
"``*Ins`` and ``*Res`` objects in ``Client`` wrap all the individual "
"values you're used to from ``NumPyClient``."
msgstr ""

#: ../../source/tutorial-customize-the-client-pytorch.ipynb:510
msgid "Step 3: Custom serialization"
msgstr ""

#: ../../source/tutorial-customize-the-client-pytorch.ipynb:512
msgid ""
"Here we will explore how to implement custom serialization with a simple "
"example."
msgstr ""

#: ../../source/tutorial-customize-the-client-pytorch.ipynb:514
msgid ""
"But first what is serialization? Serialization is just the process of "
"converting an object into raw bytes, and equally as important, "
"deserialization is the process of converting raw bytes back into an "
"object. This is very useful for network communication. Indeed, without "
"serialization, you could not just a Python object through the internet."
msgstr ""

#: ../../source/tutorial-customize-the-client-pytorch.ipynb:516
msgid ""
"Federated Learning relies heavily on internet communication for training "
"by sending Python objects back and forth between the clients and the "
"server. This means that serialization is an essential part of Federated "
"Learning."
msgstr ""

#: ../../source/tutorial-customize-the-client-pytorch.ipynb:518
msgid ""
"In the following section, we will write a basic example where instead of "
"sending a serialized version of our ``ndarray``\\ s containing our "
"parameters, we will first convert the ``ndarray`` into sparse matrices, "
"before sending them. This technique can be used to save bandwidth, as in "
"certain cases where the weights of a model are sparse (containing many 0 "
"entries), converting them to a sparse matrix can greatly improve their "
"bytesize."
msgstr ""

#: ../../source/tutorial-customize-the-client-pytorch.ipynb:521
msgid "Our custom serialization/deserialization functions"
msgstr ""

#: ../../source/tutorial-customize-the-client-pytorch.ipynb:523
msgid ""
"This is where the real serialization/deserialization will happen, "
"especially in ``ndarray_to_sparse_bytes`` for serialization and "
"``sparse_bytes_to_ndarray`` for deserialization."
msgstr ""

#: ../../source/tutorial-customize-the-client-pytorch.ipynb:525
msgid ""
"Note that we imported the ``scipy.sparse`` library in order to convert "
"our arrays."
msgstr ""

#: ../../source/tutorial-customize-the-client-pytorch.ipynb:613
msgid "Client-side"
msgstr ""

#: ../../source/tutorial-customize-the-client-pytorch.ipynb:615
msgid ""
"To be able to able to serialize our ``ndarray``\\ s into sparse "
"parameters, we will just have to call our custom functions in our "
"``flwr.client.Client``."
msgstr ""

#: ../../source/tutorial-customize-the-client-pytorch.ipynb:617
msgid ""
"Indeed, in ``get_parameters`` we need to serialize the parameters we got "
"from our network using our custom ``ndarrays_to_sparse_parameters`` "
"defined above."
msgstr ""

#: ../../source/tutorial-customize-the-client-pytorch.ipynb:619
msgid ""
"In ``fit``, we first need to deserialize the parameters coming from the "
"server using our custom ``sparse_parameters_to_ndarrays`` and then we "
"need to serialize our local results with "
"``ndarrays_to_sparse_parameters``."
msgstr ""

#: ../../source/tutorial-customize-the-client-pytorch.ipynb:621
msgid ""
"In ``evaluate``, we will only need to deserialize the global parameters "
"with our custom function."
msgstr ""

#: ../../source/tutorial-customize-the-client-pytorch.ipynb:725
msgid "Server-side"
msgstr ""

#: ../../source/tutorial-customize-the-client-pytorch.ipynb:727
msgid ""
"For this example, we will just use ``FedAvg`` as a strategy. To change "
"the serialization and deserialization here, we only need to reimplement "
"the ``evaluate`` and ``aggregate_fit`` functions of ``FedAvg``. The other"
" functions of the strategy will be inherited from the super class "
"``FedAvg``."
msgstr ""

#: ../../source/tutorial-customize-the-client-pytorch.ipynb:729
msgid "As you can see only one line as change in ``evaluate``:"
msgstr ""

#: ../../source/tutorial-customize-the-client-pytorch.ipynb:735
msgid ""
"And for ``aggregate_fit``, we will first deserialize every result we "
"received:"
msgstr ""

#: ../../source/tutorial-customize-the-client-pytorch.ipynb:744
msgid "And then serialize the aggregated result:"
msgstr ""

#: ../../source/tutorial-customize-the-client-pytorch.ipynb:903
msgid "We can now run our custom serialization example!"
msgstr ""

#: ../../source/tutorial-customize-the-client-pytorch.ipynb:932
msgid "Recap"
msgstr ""

#: ../../source/tutorial-customize-the-client-pytorch.ipynb:934
msgid ""
"In this part of the tutorial, we've seen how we can build clients by "
"subclassing either ``NumPyClient`` or ``Client``. ``NumPyClient`` is a "
"convenience abstraction that makes it easier to work with machine "
"learning libraries that have good NumPy interoperability. ``Client`` is a"
" more flexible abstraction that allows us to do things that are not "
"possible in ``NumPyClient``. In order to do so, it requires us to handle "
"parameter serialization and deserialization ourselves."
msgstr ""

#: ../../source/tutorial-customize-the-client-pytorch.ipynb:946
msgid "Next steps"
msgstr ""

#: ../../source/tutorial-customize-the-client-pytorch.ipynb:948
msgid ""
"Before you continue, make sure to join the Flower community on Slack: "
"`Join Slack <https://flower.dev/join-slack/>`__"
msgstr ""

#: ../../source/tutorial-customize-the-client-pytorch.ipynb:950
msgid ""
"There's a dedicated ``#questions`` channel if you need help, but we'd "
"also love to hear who you are in ``#introductions``!"
msgstr ""

#: ../../source/tutorial-customize-the-client-pytorch.ipynb:952
msgid ""
"This is the final part of the Flower tutorial (for now!), "
"congratulations! You're now well equipped to understand the rest of the "
"documentation. There are many topics we didn't cover in the tutorial, we "
"recommend the following resources:"
msgstr ""

#: ../../source/tutorial-customize-the-client-pytorch.ipynb:954
msgid "`Read Flower Docs <https://flower.dev/docs/>`__"
msgstr ""

#: ../../source/tutorial-customize-the-client-pytorch.ipynb:955
msgid ""
"`Check out Flower Code Examples "
"<https://github.com/adap/flower/tree/main/examples>`__"
msgstr ""

#: ../../source/tutorial-customize-the-client-pytorch.ipynb:956
msgid ""
"`Use Flower Baselines for your research "
"<https://flower.dev/docs/baselines/>`__"
msgstr ""

#: ../../source/tutorial-customize-the-client-pytorch.ipynb:957
msgid ""
"`Watch Flower Summit 2023 videos <https://flower.dev/conf/flower-"
"summit-2023/>`__"
msgstr ""

