# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2022 Adap GmbH
# This file is distributed under the same license as the Flower package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2023.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Flower \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-08-29 14:25+0200\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: fr\n"
"Language-Team: fr <LL@li.org>\n"
"Plural-Forms: nplurals=2; plural=(n > 1);\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.12.1\n"

#: ../../source/tutorial-use-a-federated-learning-strategy-pytorch.ipynb:9
msgid "Use a federated learning strategy"
msgstr ""

#: ../../source/tutorial-use-a-federated-learning-strategy-pytorch.ipynb:11
msgid ""
"Welcome to the next part of the federated learning tutorial. In previous "
"parts of this tutorial, we introduced federated learning with PyTorch and"
" Flower (`part 1 <https://flower.dev/docs/framework/tutorial-get-started-"
"with-flower-pytorch.html>`__)."
msgstr ""

#: ../../source/tutorial-use-a-federated-learning-strategy-pytorch.ipynb:13
msgid ""
"In this notebook, we'll begin to customize the federated learning system "
"we built in the introductory notebook (again, using `Flower "
"<https://flower.dev/>`__ and `PyTorch <https://pytorch.org/>`__)."
msgstr ""

#: ../../source/tutorial-use-a-federated-learning-strategy-pytorch.ipynb:15
msgid ""
"`Star Flower on GitHub <https://github.com/adap/flower>`__ ‚≠êÔ∏è and join "
"the Flower community on Slack to connect, ask questions, and get help: "
"`Join Slack <https://flower.dev/join-slack>`__ üåº We'd love to hear from "
"you in the ``#introductions`` channel! And if anything is unclear, head "
"over to the ``#questions`` channel."
msgstr ""

#: ../../source/tutorial-use-a-federated-learning-strategy-pytorch.ipynb:17
msgid "Let's move beyond FedAvg with Flower strategies!"
msgstr ""

#: ../../source/tutorial-use-a-federated-learning-strategy-pytorch.ipynb:29
msgid "Preparation"
msgstr ""

#: ../../source/tutorial-use-a-federated-learning-strategy-pytorch.ipynb:31
msgid ""
"Before we begin with the actual code, let's make sure that we have "
"everything we need."
msgstr ""

#: ../../source/tutorial-use-a-federated-learning-strategy-pytorch.ipynb:43
msgid "Installing dependencies"
msgstr ""

#: ../../source/tutorial-use-a-federated-learning-strategy-pytorch.ipynb:45
msgid "First, we install the necessary packages:"
msgstr ""

#: ../../source/tutorial-use-a-federated-learning-strategy-pytorch.ipynb:65
msgid ""
"Now that we have all dependencies installed, we can import everything we "
"need for this tutorial:"
msgstr ""

#: ../../source/tutorial-use-a-federated-learning-strategy-pytorch.ipynb:101
msgid ""
"It is possible to switch to a runtime that has GPU acceleration enabled "
"(on Google Colab: ``Runtime > Change runtime type > Hardware acclerator: "
"GPU > Save``). Note, however, that Google Colab is not always able to "
"offer GPU acceleration. If you see an error related to GPU availability "
"in one of the following sections, consider switching back to CPU-based "
"execution by setting ``DEVICE = torch.device(\"cpu\")``. If the runtime "
"has GPU acceleration enabled, you should see the output ``Training on "
"cuda``, otherwise it'll say ``Training on cpu``."
msgstr ""

#: ../../source/tutorial-use-a-federated-learning-strategy-pytorch.ipynb:114
msgid "Data loading"
msgstr ""

#: ../../source/tutorial-use-a-federated-learning-strategy-pytorch.ipynb:116
msgid ""
"Let's now load the CIFAR-10 training and test set, partition them into "
"ten smaller datasets (each split into training and validation set), and "
"wrap everything in their own ``DataLoader``. We introduce a new parameter"
" ``num_clients`` which allows us to call ``load_datasets`` with different"
" numbers of clients."
msgstr ""

#: ../../source/tutorial-use-a-federated-learning-strategy-pytorch.ipynb:167
msgid "Model training/evaluation"
msgstr ""

#: ../../source/tutorial-use-a-federated-learning-strategy-pytorch.ipynb:169
msgid ""
"Let's continue with the usual model definition (including "
"``set_parameters`` and ``get_parameters``), training and test functions:"
msgstr ""

#: ../../source/tutorial-use-a-federated-learning-strategy-pytorch.ipynb:258
msgid "Flower client"
msgstr ""

#: ../../source/tutorial-use-a-federated-learning-strategy-pytorch.ipynb:260
msgid ""
"To implement the Flower client, we (again) create a subclass of "
"``flwr.client.NumPyClient`` and implement the three methods "
"``get_parameters``, ``fit``, and ``evaluate``. Here, we also pass the "
"``cid`` to the client and use it log additional details:"
msgstr ""

#: ../../source/tutorial-use-a-federated-learning-strategy-pytorch.ipynb:309
msgid "Strategy customization"
msgstr ""

#: ../../source/tutorial-use-a-federated-learning-strategy-pytorch.ipynb:311
msgid ""
"So far, everything should look familiar if you've worked through the "
"introductory notebook. With that, we're ready to introduce a number of "
"new features."
msgstr ""

#: ../../source/tutorial-use-a-federated-learning-strategy-pytorch.ipynb:323
msgid "Server-side parameter **initialization**"
msgstr ""

#: ../../source/tutorial-use-a-federated-learning-strategy-pytorch.ipynb:325
msgid ""
"Flower, by default, initializes the global model by asking one random "
"client for the initial parameters. In many cases, we want more control "
"over parameter initialization though. Flower therefore allows you to "
"directly pass the initial parameters to the Strategy:"
msgstr ""

#: ../../source/tutorial-use-a-federated-learning-strategy-pytorch.ipynb:370
msgid ""
"Passing ``initial_parameters`` to the ``FedAvg`` strategy prevents Flower"
" from asking one of the clients for the initial parameters. If we look "
"closely, we can see that the logs do not show any calls to the "
"``FlowerClient.get_parameters`` method."
msgstr ""

#: ../../source/tutorial-use-a-federated-learning-strategy-pytorch.ipynb:382
msgid "Starting with a customized strategy"
msgstr ""

#: ../../source/tutorial-use-a-federated-learning-strategy-pytorch.ipynb:384
msgid ""
"We've seen the function ``start_simulation`` before. It accepts a number "
"of arguments, amongst them the ``client_fn`` used to create "
"``FlowerClient`` instances, the number of clients to simulate "
"``num_clients``, the number of rounds ``num_rounds``, and the strategy."
msgstr ""

#: ../../source/tutorial-use-a-federated-learning-strategy-pytorch.ipynb:386
msgid ""
"The strategy encapsulates the federated learning approach/algorithm, for "
"example, ``FedAvg`` or ``FedAdagrad``. Let's try to use a different "
"strategy this time:"
msgstr ""

#: ../../source/tutorial-use-a-federated-learning-strategy-pytorch.ipynb:424
msgid "Server-side parameter **evaluation**"
msgstr ""

#: ../../source/tutorial-use-a-federated-learning-strategy-pytorch.ipynb:426
msgid ""
"Flower can evaluate the aggregated model on the server-side or on the "
"client-side. Client-side and server-side evaluation are similar in some "
"ways, but different in others."
msgstr ""

#: ../../source/tutorial-use-a-federated-learning-strategy-pytorch.ipynb:428
msgid ""
"**Centralized Evaluation** (or *server-side evaluation*) is conceptually "
"simple: it works the same way that evaluation in centralized machine "
"learning does. If there is a server-side dataset that can be used for "
"evaluation purposes, then that's great. We can evaluate the newly "
"aggregated model after each round of training without having to send the "
"model to clients. We're also fortunate in the sense that our entire "
"evaluation dataset is available at all times."
msgstr ""

#: ../../source/tutorial-use-a-federated-learning-strategy-pytorch.ipynb:430
msgid ""
"**Federated Evaluation** (or *client-side evaluation*) is more complex, "
"but also more powerful: it doesn't require a centralized dataset and "
"allows us to evaluate models over a larger set of data, which often "
"yields more realistic evaluation results. In fact, many scenarios require"
" us to use **Federated Evaluation** if we want to get representative "
"evaluation results at all. But this power comes at a cost: once we start "
"to evaluate on the client side, we should be aware that our evaluation "
"dataset can change over consecutive rounds of learning if those clients "
"are not always available. Moreover, the dataset held by each client can "
"also change over consecutive rounds. This can lead to evaluation results "
"that are not stable, so even if we would not change the model, we'd see "
"our evaluation results fluctuate over consecutive rounds."
msgstr ""

#: ../../source/tutorial-use-a-federated-learning-strategy-pytorch.ipynb:433
msgid ""
"We've seen how federated evaluation works on the client side (i.e., by "
"implementing the ``evaluate`` method in ``FlowerClient``). Now let's see "
"how we can evaluate aggregated model parameters on the server-side:"
msgstr ""

#: ../../source/tutorial-use-a-federated-learning-strategy-pytorch.ipynb:490
msgid "Sending/receiving arbitrary values to/from clients"
msgstr ""

#: ../../source/tutorial-use-a-federated-learning-strategy-pytorch.ipynb:492
msgid ""
"In some situations, we want to configure client-side execution (training,"
" evaluation) from the server-side. One example for that is the server "
"asking the clients to train for a certain number of local epochs. Flower "
"provides a way to send configuration values from the server to the "
"clients using a dictionary. Let's look at an example where the clients "
"receive values from the server through the ``config`` parameter in "
"``fit`` (``config`` is also available in ``evaluate``). The ``fit`` "
"method receives the configuration dictionary through the ``config`` "
"parameter and can then read values from this dictionary. In this example,"
" it reads ``server_round`` and ``local_epochs`` and uses those values to "
"improve the logging and configure the number of local training epochs:"
msgstr ""

#: ../../source/tutorial-use-a-federated-learning-strategy-pytorch.ipynb:546
msgid ""
"So how can we send this config dictionary from server to clients? The "
"built-in Flower Strategies provide way to do this, and it works similarly"
" to the way server-side evaluation works. We provide a function to the "
"strategy, and the strategy calls this function for every round of "
"federated learning:"
msgstr ""

#: ../../source/tutorial-use-a-federated-learning-strategy-pytorch.ipynb:576
msgid ""
"Next, we'll just pass this function to the FedAvg strategy before "
"starting the simulation:"
msgstr ""

#: ../../source/tutorial-use-a-federated-learning-strategy-pytorch.ipynb:613
msgid ""
"As we can see, the client logs now include the current round of federated"
" learning (which they read from the ``config`` dictionary). We can also "
"configure local training to run for one epoch during the first and second"
" round of federated learning, and then for two epochs during the third "
"round."
msgstr ""

#: ../../source/tutorial-use-a-federated-learning-strategy-pytorch.ipynb:615
msgid ""
"Clients can also return arbitrary values to the server. To do so, they "
"return a dictionary from ``fit`` and/or ``evaluate``. We have seen and "
"used this concept throughout this notebook without mentioning it "
"explicitly: our ``FlowerClient`` returns a dictionary containing a custom"
" key/value pair as the third return value in ``evaluate``."
msgstr ""

#: ../../source/tutorial-use-a-federated-learning-strategy-pytorch.ipynb:627
msgid "Scaling federated learning"
msgstr ""

#: ../../source/tutorial-use-a-federated-learning-strategy-pytorch.ipynb:629
msgid ""
"As a last step in this notebook, let's see how we can use Flower to "
"experiment with a large number of clients."
msgstr ""

#: ../../source/tutorial-use-a-federated-learning-strategy-pytorch.ipynb:651
#, python-format
msgid ""
"We now have 1000 partitions, each holding 45 training and 5 validation "
"examples. Given that the number of training examples on each client is "
"quite small, we should probably train the model a bit longer, so we "
"configure the clients to perform 3 local training epochs. We should also "
"adjust the fraction of clients selected for training during each round "
"(we don't want all 1000 clients participating in every round), so we "
"adjust ``fraction_fit`` to ``0.05``, which means that only 5% of "
"available clients (so 50 clients) will be selected for training each "
"round:"
msgstr ""

#: ../../source/tutorial-use-a-federated-learning-strategy-pytorch.ipynb:697
msgid "Recap"
msgstr ""

#: ../../source/tutorial-use-a-federated-learning-strategy-pytorch.ipynb:699
msgid ""
"In this notebook, we've seen how we can gradually enhance our system by "
"customizing the strategy, initializing parameters on the server side, "
"choosing a different strategy, and evaluating models on the server-side. "
"That's quite a bit of flexibility with so little code, right?"
msgstr ""

#: ../../source/tutorial-use-a-federated-learning-strategy-pytorch.ipynb:701
msgid ""
"In the later sections, we've seen how we can communicate arbitrary values"
" between server and clients to fully customize client-side execution. "
"With that capability, we built a large-scale Federated Learning "
"simulation using the Flower Virtual Client Engine and ran an experiment "
"involving 1000 clients in the same workload - all in a Jupyter Notebook!"
msgstr ""

#: ../../source/tutorial-use-a-federated-learning-strategy-pytorch.ipynb:713
msgid "Next steps"
msgstr ""

#: ../../source/tutorial-use-a-federated-learning-strategy-pytorch.ipynb:715
msgid ""
"Before you continue, make sure to join the Flower community on Slack: "
"`Join Slack <https://flower.dev/join-slack/>`__"
msgstr ""

#: ../../source/tutorial-use-a-federated-learning-strategy-pytorch.ipynb:717
msgid ""
"There's a dedicated ``#questions`` channel if you need help, but we'd "
"also love to hear who you are in ``#introductions``!"
msgstr ""

#: ../../source/tutorial-use-a-federated-learning-strategy-pytorch.ipynb:719
msgid ""
"The `Flower Federated Learning Tutorial - Part 3 "
"<https://flower.dev/docs/framework/tutorial-build-a-strategy-from-"
"scratch-pytorch.html>`__ shows how to build a fully custom ``Strategy`` "
"from scratch."
msgstr ""

