# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2022 Adap GmbH
# This file is distributed under the same license as the Flower package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2023.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Flower \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-08-09 15:41+0200\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: en\n"
"Language-Team: en <LL@li.org>\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.12.1\n"

#: ../../source/quickstart-jax.rst:5
msgid "Quickstart JAX"
msgstr ""

#: ../../source/quickstart-jax.rst:7
msgid ""
"This tutorial will show you how to use Flower to build a federated "
"version of an existing JAX workload. We are using JAX to train a linear "
"regression model on a scikit-learn dataset. We will structure the example"
" similar to our `PyTorch - From Centralized To Federated "
"<https://github.com/adap/flower/blob/main/examples/pytorch-from-"
"centralized-to-federated>`_ walkthrough. First, we build a centralized "
"training approach based on the `Linear Regression with JAX "
"<https://coax.readthedocs.io/en/latest/examples/linear_regression/jax.html>`_"
" tutorial`. Then, we build upon the centralized training code to run the "
"training in a federated fashion."
msgstr ""

#: ../../source/quickstart-jax.rst:13
msgid ""
"Before we start building our JAX example, we need install the packages "
":code:`jax`, :code:`jaxlib`, :code:`scikit-learn`, and :code:`flwr`:"
msgstr ""

#: ../../source/quickstart-jax.rst:21
msgid "Linear Regression with JAX"
msgstr ""

#: ../../source/quickstart-jax.rst:23
msgid ""
"We begin with a brief description of the centralized training code based "
"on a :code:`Linear Regression` model. If you want a more in-depth "
"explanation of what's going on then have a look at the official `JAX "
"documentation <https://jax.readthedocs.io/>`_."
msgstr ""

#: ../../source/quickstart-jax.rst:26
msgid ""
"Let's create a new file called :code:`jax_training.py` with all the "
"components required for a traditional (centralized) linear regression "
"training. First, the JAX packages :code:`jax` and :code:`jaxlib` need to "
"be imported. In addition, we need to import :code:`sklearn` since we use "
":code:`make_regression` for the dataset and :code:`train_test_split` to "
"split the dataset into a training and test set. You can see that we do "
"not yet import the :code:`flwr` package for federated learning. This will"
" be done later."
msgstr ""

#: ../../source/quickstart-jax.rst:40
msgid ""
"The :code:`load_data()` function loads the mentioned training and test "
"sets."
msgstr ""

#: ../../source/quickstart-jax.rst:50
msgid ""
"The model architecture (a very simple :code:`Linear Regression` model) is"
" defined in :code:`load_model()`."
msgstr ""

#: ../../source/quickstart-jax.rst:62
msgid ""
"We now need to define the training (function :code:`train()`), which "
"loops over the training set and measures the loss (function "
":code:`loss_fn()`) for each batch of training examples. The loss function"
" is separate since JAX takes derivatives with a :code:`grad()` function "
"(defined in the :code:`main()` function and called in :code:`train()`)."
msgstr ""

#: ../../source/quickstart-jax.rst:80
msgid ""
"The evaluation of the model is defined in the function "
":code:`evaluation()`. The function takes all test examples and measures "
"the loss of the linear regression model."
msgstr ""

#: ../../source/quickstart-jax.rst:91
msgid ""
"Having defined the data loading, model architecture, training, and "
"evaluation we can put everything together and train our model using JAX. "
"As already mentioned, the :code:`jax.grad()` function is defined in "
":code:`main()` and passed to :code:`train()`."
msgstr ""

#: ../../source/quickstart-jax.rst:108
msgid "You can now run your (centralized) JAX linear regression workload:"
msgstr ""

#: ../../source/quickstart-jax.rst:114
msgid ""
"So far this should all look fairly familiar if you've used JAX before. "
"Let's take the next step and use what we've built to create a simple "
"federated learning system consisting of one server and two clients."
msgstr ""

#: ../../source/quickstart-jax.rst:118
msgid "JAX meets Flower"
msgstr ""

#: ../../source/quickstart-jax.rst:120
msgid ""
"The concept of federating an existing workload is always the same and "
"easy to understand. We have to start a *server* and then use the code in "
":code:`jax_training.py` for the *clients* that are connected to the "
"*server*. The *server* sends model parameters to the clients. The "
"*clients* run the training and update the parameters. The updated "
"parameters are sent back to the *server*, which averages all received "
"parameter updates. This describes one round of the federated learning "
"process, and we repeat this for multiple rounds."
msgstr ""

#: ../../source/quickstart-jax.rst:126
msgid ""
"Our example consists of one *server* and two *clients*. Let's set up "
":code:`server.py` first. The *server* needs to import the Flower package "
":code:`flwr`. Next, we use the :code:`start_server` function to start a "
"server and tell it to perform three rounds of federated learning."
msgstr ""

#: ../../source/quickstart-jax.rst:136
msgid "We can already start the *server*:"
msgstr ""

#: ../../source/quickstart-jax.rst:142
msgid ""
"Finally, we will define our *client* logic in :code:`client.py` and build"
" upon the previously defined JAX training in :code:`jax_training.py`. Our"
" *client* needs to import :code:`flwr`, but also :code:`jax` and "
":code:`jaxlib` to update the parameters on our JAX model:"
msgstr ""

#: ../../source/quickstart-jax.rst:157
msgid ""
"Implementing a Flower *client* basically means implementing a subclass of"
" either :code:`flwr.client.Client` or :code:`flwr.client.NumPyClient`. "
"Our implementation will be based on :code:`flwr.client.NumPyClient` and "
"we'll call it :code:`FlowerClient`. :code:`NumPyClient` is slightly "
"easier to implement than :code:`Client` if you use a framework with good "
"NumPy interoperability (like JAX) because it avoids some of the "
"boilerplate that would otherwise be necessary. :code:`FlowerClient` needs"
" to implement four methods, two methods for getting/setting model "
"parameters, one method for training the model, and one method for testing"
" the model:"
msgstr ""

#: ../../source/quickstart-jax.rst:164
msgid ":code:`set_parameters (optional)`"
msgstr ""

#: ../../source/quickstart-jax.rst:163
msgid ""
"set the model parameters on the local model that are received from the "
"server"
msgstr ""

#: ../../source/quickstart-jax.rst:164
msgid "transform parameters to NumPy :code:`ndarray`'s"
msgstr ""

#: ../../source/quickstart-jax.rst:165
msgid ""
"loop over the list of model parameters received as NumPy "
":code:`ndarray`'s (think list of neural network layers)"
msgstr ""

#: ../../source/quickstart-jax.rst:166
msgid ":code:`get_parameters`"
msgstr ""

#: ../../source/quickstart-jax.rst:167
msgid ""
"get the model parameters and return them as a list of NumPy "
":code:`ndarray`'s (which is what :code:`flwr.client.NumPyClient` expects)"
msgstr ""

#: ../../source/quickstart-jax.rst:170
msgid ":code:`fit`"
msgstr ""

#: ../../source/quickstart-jax.rst:169 ../../source/quickstart-jax.rst:173
msgid ""
"update the parameters of the local model with the parameters received "
"from the server"
msgstr ""

#: ../../source/quickstart-jax.rst:170
msgid "train the model on the local training set"
msgstr ""

#: ../../source/quickstart-jax.rst:171
msgid "get the updated local model parameters and return them to the server"
msgstr ""

#: ../../source/quickstart-jax.rst:175
msgid ":code:`evaluate`"
msgstr ""

#: ../../source/quickstart-jax.rst:174
msgid "evaluate the updated model on the local test set"
msgstr ""

#: ../../source/quickstart-jax.rst:175
msgid "return the local loss to the server"
msgstr ""

#: ../../source/quickstart-jax.rst:177
msgid ""
"The challenging part is to transform the JAX model parameters from "
":code:`DeviceArray` to :code:`NumPy ndarray` to make them compatible with"
" `NumPyClient`."
msgstr ""

#: ../../source/quickstart-jax.rst:179
msgid ""
"The two :code:`NumPyClient` methods :code:`fit` and :code:`evaluate` make"
" use of the functions :code:`train()` and :code:`evaluate()` previously "
"defined in :code:`jax_training.py`. So what we really do here is we tell "
"Flower through our :code:`NumPyClient` subclass which of our already "
"defined functions to call for training and evaluation. We included type "
"annotations to give you a better understanding of the data types that get"
" passed around."
msgstr ""

#: ../../source/quickstart-jax.rst:248
msgid "Having defined the federation process, we can run it."
msgstr ""

#: ../../source/quickstart-jax.rst:271
msgid "And that's it. You can now open two additional terminal windows and run"
msgstr ""

#: ../../source/quickstart-jax.rst:277
msgid ""
"in each window (make sure that the server is still running before you do "
"so) and see your JAX project run federated learning across two clients. "
"Congratulations!"
msgstr ""

#: ../../source/quickstart-jax.rst:280
msgid "Next Steps"
msgstr ""

#: ../../source/quickstart-jax.rst:282
msgid ""
"The source code of this example was improved over time and can be found "
"here: `Quickstart JAX <https://github.com/adap/flower/blob/main/examples"
"/quickstart-jax>`_. Our example is somewhat over-simplified because both "
"clients load the same dataset."
msgstr ""

#: ../../source/quickstart-jax.rst:285
msgid ""
"You're now prepared to explore this topic further. How about using a more"
" sophisticated model or using a different dataset? How about adding more "
"clients?"
msgstr ""

