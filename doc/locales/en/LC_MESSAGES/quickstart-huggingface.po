# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2022 Adap GmbH
# This file is distributed under the same license as the Flower package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2023.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Flower \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-08-09 15:41+0200\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: en\n"
"Language-Team: en <LL@li.org>\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.12.1\n"

#: ../../source/quickstart-huggingface.rst:5
msgid "Quickstart ðŸ¤— Transformers"
msgstr ""

#: ../../source/quickstart-huggingface.rst:7
msgid ""
"Let's build a federated learning system using Hugging Face Transformers "
"and Flower!"
msgstr ""

#: ../../source/quickstart-huggingface.rst:9
msgid ""
"We will leverage Hugging Face to federate the training of language models"
" over multiple clients using Flower. More specifically, we will fine-tune"
" a pre-trained Transformer model (distilBERT) for sequence classification"
" over a dataset of IMDB ratings. The end goal is to detect if a movie "
"rating is positive or negative."
msgstr ""

#: ../../source/quickstart-huggingface.rst:15
msgid "Dependencies"
msgstr ""

#: ../../source/quickstart-huggingface.rst:17
msgid ""
"To follow along this tutorial you will need to install the following "
"packages: :code:`datasets`, :code:`evaluate`, :code:`flwr`, "
":code:`torch`, and :code:`transformers`. This can be done using "
":code:`pip`:"
msgstr ""

#: ../../source/quickstart-huggingface.rst:27
msgid "Standard Hugging Face workflow"
msgstr ""

#: ../../source/quickstart-huggingface.rst:30
msgid "Handling the data"
msgstr ""

#: ../../source/quickstart-huggingface.rst:32
msgid ""
"To fetch the IMDB dataset, we will use Hugging Face's :code:`datasets` "
"library. We then need to tokenize the data and create :code:`PyTorch` "
"dataloaders, this is all done in the :code:`load_data` function:"
msgstr ""

#: ../../source/quickstart-huggingface.rst:78
msgid "Training and testing the model"
msgstr ""

#: ../../source/quickstart-huggingface.rst:80
msgid ""
"Once we have a way of creating our trainloader and testloader, we can "
"take care of the training and testing. This is very similar to any "
":code:`PyTorch` training or testing loop:"
msgstr ""

#: ../../source/quickstart-huggingface.rst:118
msgid "Creating the model itself"
msgstr ""

#: ../../source/quickstart-huggingface.rst:120
msgid ""
"To create the model itself, we will just load the pre-trained distillBERT"
" model using Hugging Faceâ€™s :code:`AutoModelForSequenceClassification` :"
msgstr ""

#: ../../source/quickstart-huggingface.rst:133
msgid "Federating the example"
msgstr ""

#: ../../source/quickstart-huggingface.rst:136
msgid "Creating the IMDBClient"
msgstr ""

#: ../../source/quickstart-huggingface.rst:138
msgid ""
"To federate our example to multiple clients, we first need to write our "
"Flower client class (inheriting from :code:`flwr.client.NumPyClient`). "
"This is very easy, as our model is a standard :code:`PyTorch` model:"
msgstr ""

#: ../../source/quickstart-huggingface.rst:166
msgid ""
"The :code:`get_parameters` function lets the server get the client's "
"parameters. Inversely, the :code:`set_parameters` function allows the "
"server to send its parameters to the client. Finally, the :code:`fit` "
"function trains the model locally for the client, and the "
":code:`evaluate` function tests the model locally and returns the "
"relevant metrics."
msgstr ""

#: ../../source/quickstart-huggingface.rst:172
msgid "Starting the server"
msgstr ""

#: ../../source/quickstart-huggingface.rst:174
msgid ""
"Now that we have a way to instantiate clients, we need to create our "
"server in order to aggregate the results. Using Flower, this can be done "
"very easily by first choosing a strategy (here, we are using "
":code:`FedAvg`, which will define the global weights as the average of "
"all the clients' weights at each round) and then using the "
":code:`flwr.server.start_server` function:"
msgstr ""

#: ../../source/quickstart-huggingface.rst:202
msgid ""
"The :code:`weighted_average` function is there to provide a way to "
"aggregate the metrics distributed amongst the clients (basically this "
"allows us to display a nice average accuracy and loss for every round)."
msgstr ""

#: ../../source/quickstart-huggingface.rst:206
msgid "Putting everything together"
msgstr ""

#: ../../source/quickstart-huggingface.rst:208
msgid "We can now start client instances using:"
msgstr ""

#: ../../source/quickstart-huggingface.rst:218
msgid ""
"And they will be able to connect to the server and start the federated "
"training."
msgstr ""

#: ../../source/quickstart-huggingface.rst:220
msgid ""
"If you want to check out everything put together, you should check out "
"the full code example: [https://github.com/adap/flower/tree/main/examples"
"/quickstart-"
"huggingface](https://github.com/adap/flower/tree/main/examples"
"/quickstart-huggingface)."
msgstr ""

#: ../../source/quickstart-huggingface.rst:224
msgid ""
"Of course, this is a very basic example, and a lot can be added or "
"modified, it was just to showcase how simply we could federate a Hugging "
"Face workflow using Flower."
msgstr ""

#: ../../source/quickstart-huggingface.rst:227
msgid ""
"Note that in this example we used :code:`PyTorch`, but we could have very"
" well used :code:`TensorFlow`."
msgstr ""

