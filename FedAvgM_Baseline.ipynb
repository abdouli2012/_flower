{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gubertoli/flower/blob/baselines_fedavgm/FedAvgM_Baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFdIaOdbHvAo"
      },
      "source": [
        "# FedAvgM Baseline\n",
        "\n",
        "Reference:\n",
        "\n",
        "- **FedAvgM paper**: Hsu, T. M. H., Qi, H., & Brown, M. (2019). Measuring the effects of non-identical data distribution for federated visual classification. arXiv preprint arXiv:1909.06335. [Link](https://arxiv.org/abs/1909.06335)\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRzB3rc58j4o"
      },
      "outputs": [],
      "source": [
        "!pip install -q flwr[\"simulation\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S38RZUzE9CJX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Make TensorFlow logs less verbose\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "\n",
        "import math\n",
        "import tensorflow as tf\n",
        "import flwr as fl\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjWed-nDF1_v",
        "outputId": "364d09ed-a001-482d-8ccf-ed93039d60d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.4.0\n"
          ]
        }
      ],
      "source": [
        "print(fl.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simulation Parameters\n",
        "### Variables\n",
        "- **FEMNIST**: Boolean variable, if set True will run simulation with the FEMNIST dataset, if False runs with CIFAR-10\n",
        "- **CONCENTRATION**: Represents the 8 possible values used in the FedAvgM paper (referred as \"α\" alpha), this concentration parameter is used by the Direchlet distribution to create non-IID distributions between the clients. According to Figure 2 from the paper, the applicable values are [100., 10., 1., 0.5, 0.2, 0.1, 0.05, 0.0]. The understanding for this variable is that α→∞ all clients have identical distribution, and α→𝟢 each client hold samples from only one class.\n",
        "- **REPORTING_FRACTION**: It is the % of clients participating in the federated learning process, in the paper it is referred as \"C\" and have the values [0.05, 0.1, 0.4]\n",
        "\n",
        "\n",
        "### Constants\n",
        "- **NUM_CLIENTS**: Set to 100\n",
        "- **COMMUNICATION_ROUNDS**: 10000\n",
        "- **MOMENTUM**: 0.9 - According to the FedAvgM paper could assume the values [0, 0.7, 0.9, 0.97, 0.99, 0.997], but does not change in this evaluation/simulation\n",
        "- **LEARNING_RATE**: The learning rate for the FedAvgM aggregation algorithm (server-side), it is constant and 1.0\n",
        "- **BATCH SIZE**: referred as \"B\" in the paper, does not change in this evaluation, with the value 64"
      ],
      "metadata": {
        "id": "7MgOBfro7iYw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wv7wFIBPHFGv"
      },
      "outputs": [],
      "source": [
        "FEMNIST = False     # If False, FedAvgM runs with CIFAR-10\n",
        "NUM_CLIENTS = 100   # The paper runs with 100 clients, does not change in this evaluation\n",
        "CONCENTRATION = 1.  # The papers uses 8 values (referred as \"α\") / From Figure 2: [100., 10., 1., 0.5, 0.2, 0.1, 0.05, 0.0]\n",
        "\n",
        "# Server-side parameters\n",
        "REPORTING_FRACTION = 0.05     # [0.05, 0.1, 0.4]   # referred as \"C\" in the paper\n",
        "COMMUNICATION_ROUNDS = 10000  # does not change in this evaluation\n",
        "MOMENTUM = 0.9                # [0, 0.7, 0.9, 0.97, 0.99, 0.997] - does not change in this evaluation\n",
        "LEARNING_RATE = 1.0           # does not change in this evaluation\n",
        "\n",
        "# Client-side parameters\n",
        "BATCH_SIZE = 64     # referred as \"B\" in the paper, does not change in this evaluation\n",
        "LOCAL_EPOCHS = 1    # the paper uses local epoch of 1 or 5 (referred as \"E\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvEoAsT2GOnP"
      },
      "outputs": [],
      "source": [
        "# From Flower common functions\n",
        "\n",
        "from typing import List, Optional, Tuple, Union\n",
        "from numpy.random import BitGenerator, Generator, SeedSequence\n",
        "\n",
        "XY = Tuple[np.ndarray, np.ndarray]\n",
        "XYList = List[XY]\n",
        "PartitionedDataset = Tuple[XYList, XYList]\n",
        "\n",
        "np.random.seed(2020)\n",
        "\n",
        "def shuffle(x: np.ndarray, y: np.ndarray) -> XY:\n",
        "    \"\"\"Shuffle x and y.\"\"\"\n",
        "    idx = np.random.permutation(len(x))\n",
        "    return x[idx], y[idx]\n",
        "\n",
        "\n",
        "def sort_by_label(x: np.ndarray, y: np.ndarray) -> XY:\n",
        "    \"\"\"Sort by label.\n",
        "\n",
        "    Assuming two labels and four examples the resulting label order\n",
        "    would be 1,1,2,2\n",
        "    \"\"\"\n",
        "    idx = np.argsort(y, axis=0).reshape((y.shape[0]))\n",
        "    return (x[idx], y[idx])\n",
        "\n",
        "def split_array_at_indices(\n",
        "    x: np.ndarray, split_idx: np.ndarray\n",
        ") -> List[List[np.ndarray]]:\n",
        "    \"\"\"Splits an array `x` into list of elements using starting indices from\n",
        "    `split_idx`.\n",
        "\n",
        "        This function should be used with `unique_indices` from `np.unique()` after\n",
        "        sorting by label.\n",
        "\n",
        "    Args:\n",
        "        x (np.ndarray): Original array of dimension (N,a,b,c,...)\n",
        "        split_idx (np.ndarray): 1-D array contaning increasing number of\n",
        "            indices to be used as partitions. Initial value must be zero. Last value\n",
        "            must be less than N.\n",
        "\n",
        "    Returns:\n",
        "        List[List[np.ndarray]]: List of list of samples.\n",
        "    \"\"\"\n",
        "\n",
        "    if split_idx.ndim != 1:\n",
        "        raise ValueError(\"Variable `split_idx` must be a 1-D numpy array.\")\n",
        "    if split_idx.dtype != np.int64:\n",
        "        raise ValueError(\"Variable `split_idx` must be of type np.int64.\")\n",
        "    if split_idx[0] != 0:\n",
        "        raise ValueError(\"First value of `split_idx` must be 0.\")\n",
        "    if split_idx[-1] >= x.shape[0]:\n",
        "        raise ValueError(\n",
        "            \"\"\"Last value in `split_idx` must be less than\n",
        "            the number of samples in `x`.\"\"\"\n",
        "        )\n",
        "    if not np.all(split_idx[:-1] <= split_idx[1:]):\n",
        "        raise ValueError(\"Items in `split_idx` must be in increasing order.\")\n",
        "\n",
        "    num_splits: int = len(split_idx)\n",
        "    split_idx = np.append(split_idx, x.shape[0])\n",
        "\n",
        "    list_samples_split: List[List[np.ndarray]] = [[] for _ in range(num_splits)]\n",
        "    for j in range(num_splits):\n",
        "        tmp_x = x[split_idx[j] : split_idx[j + 1]]  # noqa: E203\n",
        "        for sample in tmp_x:\n",
        "            list_samples_split[j].append(sample)\n",
        "\n",
        "    return list_samples_split\n",
        "\n",
        "\n",
        "def sample_without_replacement(\n",
        "    distribution: np.ndarray,\n",
        "    list_samples: List[List[np.ndarray]],\n",
        "    num_samples: int,\n",
        "    empty_classes: List[bool],\n",
        ") -> Tuple[XY, List[bool]]:\n",
        "    \"\"\"Samples from a list without replacement using a given distribution.\n",
        "\n",
        "    Args:\n",
        "        distribution (np.ndarray): Distribution used for sampling.\n",
        "        list_samples(List[List[np.ndarray]]): List of samples.\n",
        "        num_samples (int): Total number of items to be sampled.\n",
        "        empty_classes (List[bool]): List of booleans indicating which classes are empty.\n",
        "            This is useful to differentiate which classes should still be sampled.\n",
        "\n",
        "    Returns:\n",
        "        XY: Dataset contaning samples\n",
        "        List[bool]: empty_classes.\n",
        "    \"\"\"\n",
        "    if np.sum([len(x) for x in list_samples]) < num_samples:\n",
        "        raise ValueError(\n",
        "            \"\"\"Number of samples in `list_samples` is less than `num_samples`\"\"\"\n",
        "        )\n",
        "\n",
        "    # Make sure empty classes are not sampled\n",
        "    # and solves for rare cases where\n",
        "    if not empty_classes:\n",
        "        empty_classes = len(distribution) * [False]\n",
        "\n",
        "    distribution = exclude_classes_and_normalize(\n",
        "        distribution=distribution, exclude_dims=empty_classes\n",
        "    )\n",
        "\n",
        "    data: List[np.ndarray] = []\n",
        "    target: List[np.ndarray] = []\n",
        "\n",
        "    for _ in range(num_samples):\n",
        "        sample_class = np.where(np.random.multinomial(1, distribution) == 1)[0][0]\n",
        "        sample: np.ndarray = list_samples[sample_class].pop()\n",
        "\n",
        "        data.append(sample)\n",
        "        target.append(sample_class)\n",
        "\n",
        "        # If last sample of the class was drawn, then set the\n",
        "        #  probability density function (PDF) to zero for that class.\n",
        "        if len(list_samples[sample_class]) == 0:\n",
        "            empty_classes[sample_class] = True\n",
        "            # Be careful to distinguish between classes that had zero probability\n",
        "            # and classes that are now empty\n",
        "            distribution = exclude_classes_and_normalize(\n",
        "                distribution=distribution, exclude_dims=empty_classes\n",
        "            )\n",
        "    data_array: np.ndarray = np.concatenate([data], axis=0)\n",
        "    target_array: np.ndarray = np.array(target, dtype=np.int64)\n",
        "\n",
        "    return (data_array, target_array), empty_classes\n",
        "\n",
        "\n",
        "def exclude_classes_and_normalize(\n",
        "    distribution: np.ndarray, exclude_dims: List[bool], eps: float = 1e-5\n",
        ") -> np.ndarray:\n",
        "    \"\"\"Excludes classes from a distribution.\n",
        "\n",
        "    This function is particularly useful when sampling without replacement.\n",
        "    Classes for which no sample is available have their probabilities are set to 0.\n",
        "    Classes that had probabilities originally set to 0 are incremented with\n",
        "     `eps` to allow sampling from remaining items.\n",
        "\n",
        "    Args:\n",
        "        distribution (np.array): Distribution being used.\n",
        "        exclude_dims (List[bool]): Dimensions to be excluded.\n",
        "        eps (float, optional): Small value to be addad to non-excluded dimensions.\n",
        "            Defaults to 1e-5.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Normalized distributions.\n",
        "    \"\"\"\n",
        "    if np.any(distribution < 0) or (not np.isclose(np.sum(distribution), 1.0)):\n",
        "        raise ValueError(\"distribution must sum to 1 and have only positive values.\")\n",
        "\n",
        "    if distribution.size != len(exclude_dims):\n",
        "        raise ValueError(\n",
        "            \"\"\"Length of distribution must be equal\n",
        "            to the length `exclude_dims`.\"\"\"\n",
        "        )\n",
        "    if eps < 0:\n",
        "        raise ValueError(\"\"\"The value of `eps` must be positive and small.\"\"\")\n",
        "\n",
        "    distribution[[not x for x in exclude_dims]] += eps\n",
        "    distribution[exclude_dims] = 0.0\n",
        "    sum_rows = np.sum(distribution) + np.finfo(float).eps\n",
        "    distribution = distribution / sum_rows\n",
        "\n",
        "    return distribution\n",
        "\n",
        "\n",
        "def create_lda_partitions(\n",
        "    dataset: XY,\n",
        "    dirichlet_dist: Optional[np.ndarray] = None,\n",
        "    num_partitions: int = 100,\n",
        "    concentration: Union[float, np.ndarray, List[float]] = 0.5,\n",
        "    accept_imbalanced: bool = False,\n",
        "    seed: Optional[Union[int, SeedSequence, BitGenerator, Generator]] = None,\n",
        ") -> Tuple[XYList, np.ndarray]:\n",
        "    \"\"\"Create imbalanced non-iid partitions using Latent Dirichlet Allocation\n",
        "    (LDA) without resampling.\n",
        "\n",
        "    Args:\n",
        "        dataset (XY): Dataset containing samples X and labels Y.\n",
        "        dirichlet_dist (numpy.ndarray, optional): previously generated distribution to\n",
        "            be used. This is useful when applying the same distribution for train and\n",
        "            validation sets.\n",
        "        num_partitions (int, optional): Number of partitions to be created.\n",
        "            Defaults to 100.\n",
        "        concentration (float, np.ndarray, List[float]): Dirichlet Concentration\n",
        "            (:math:`\\\\alpha`) parameter. Set to float('inf') to get uniform partitions.\n",
        "            An :math:`\\\\alpha \\\\to \\\\Inf` generates uniform distributions over classes.\n",
        "            An :math:`\\\\alpha \\\\to 0.0` generates one class per client. Defaults to 0.5.\n",
        "        accept_imbalanced (bool): Whether or not to accept imbalanced output classes.\n",
        "            Default False.\n",
        "        seed (None, int, SeedSequence, BitGenerator, Generator):\n",
        "            A seed to initialize the BitGenerator for generating the Dirichlet\n",
        "            distribution. This is defined in Numpy's official documentation as follows:\n",
        "            If None, then fresh, unpredictable entropy will be pulled from the OS.\n",
        "            One may also pass in a SeedSequence instance.\n",
        "            Additionally, when passed a BitGenerator, it will be wrapped by Generator.\n",
        "            If passed a Generator, it will be returned unaltered.\n",
        "            See official Numpy Documentation for further details.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[XYList, numpy.ndarray]: List of XYList containing partitions\n",
        "            for each dataset and the dirichlet probability density functions.\n",
        "    \"\"\"\n",
        "    # pylint: disable=too-many-arguments,too-many-locals\n",
        "\n",
        "    x, y = dataset\n",
        "    x, y = shuffle(x, y)\n",
        "    x, y = sort_by_label(x, y)\n",
        "\n",
        "    if (x.shape[0] % num_partitions) and (not accept_imbalanced):\n",
        "        raise ValueError(\n",
        "            \"\"\"Total number of samples must be a multiple of `num_partitions`.\n",
        "               If imbalanced classes are allowed, set\n",
        "               `accept_imbalanced=True`.\"\"\"\n",
        "        )\n",
        "\n",
        "    num_samples = num_partitions * [0]\n",
        "    for j in range(x.shape[0]):\n",
        "        num_samples[j % num_partitions] += 1\n",
        "\n",
        "    # Get number of classes and verify if they matching with\n",
        "    classes, start_indices = np.unique(y, return_index=True)\n",
        "\n",
        "    # Make sure that concentration is np.array and\n",
        "    # check if concentration is appropriate\n",
        "    concentration = np.asarray(concentration)\n",
        "\n",
        "    # Check if concentration is Inf, if so create uniform partitions\n",
        "    partitions: List[XY] = [(_, _) for _ in range(num_partitions)]\n",
        "    if float(\"inf\") in concentration:\n",
        "        partitions = create_partitions(\n",
        "            unpartitioned_dataset=(x, y),\n",
        "            iid_fraction=1.0,\n",
        "            num_partitions=num_partitions,\n",
        "        )\n",
        "        dirichlet_dist = get_partitions_distributions(partitions)[0]\n",
        "\n",
        "        return partitions, dirichlet_dist\n",
        "\n",
        "    if concentration.size == 1:\n",
        "        concentration = np.repeat(concentration, classes.size)\n",
        "    elif concentration.size != classes.size:  # Sequence\n",
        "        raise ValueError(\n",
        "            f\"The size of the provided concentration ({concentration.size}) \",\n",
        "            f\"must be either 1 or equal number of classes {classes.size})\",\n",
        "        )\n",
        "\n",
        "    # Split into list of list of samples per class\n",
        "    list_samples_per_class: List[List[np.ndarray]] = split_array_at_indices(\n",
        "        x, start_indices\n",
        "    )\n",
        "\n",
        "    if dirichlet_dist is None:\n",
        "        dirichlet_dist = np.random.default_rng(seed).dirichlet(\n",
        "            alpha=concentration, size=num_partitions\n",
        "        )\n",
        "\n",
        "    if dirichlet_dist.size != 0:\n",
        "        if dirichlet_dist.shape != (num_partitions, classes.size):\n",
        "            raise ValueError(\n",
        "                f\"\"\"The shape of the provided dirichlet distribution\n",
        "                 ({dirichlet_dist.shape}) must match the provided number\n",
        "                  of partitions and classes ({num_partitions},{classes.size})\"\"\"\n",
        "            )\n",
        "\n",
        "    # Assuming balanced distribution\n",
        "    empty_classes = classes.size * [False]\n",
        "    for partition_id in range(num_partitions):\n",
        "        partitions[partition_id], empty_classes = sample_without_replacement(\n",
        "            distribution=dirichlet_dist[partition_id].copy(),\n",
        "            list_samples=list_samples_per_class,\n",
        "            num_samples=num_samples[partition_id],\n",
        "            empty_classes=empty_classes,\n",
        "        )\n",
        "\n",
        "    return partitions, dirichlet_dist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODd6-mv9GToG"
      },
      "outputs": [],
      "source": [
        "if FEMNIST == True:\n",
        "  (x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
        "  x_train = x_train.astype('float32') / 255\n",
        "  x_test = x_test.astype('float32') / 255\n",
        "  input_shape = x_train.shape[1:]\n",
        "  num_classes = len(np.unique(y_train))\n",
        "else:\n",
        "  (x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "  x_train = x_train.astype('float32') / 255\n",
        "  x_test = x_test.astype('float32') / 255\n",
        "  input_shape = x_train.shape[1:]\n",
        "  num_classes = len(np.unique(y_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ePeXx6fHHLqg"
      },
      "outputs": [],
      "source": [
        "dataset = [x_train, y_train]\n",
        "def partition(concentration=CONCENTRATION):\n",
        "  partitions, b = create_lda_partitions(dataset, num_partitions=NUM_CLIENTS, concentration=concentration * num_classes, seed=1234)\n",
        "  return partitions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "EqnIQOoXIEyV"
      },
      "outputs": [],
      "source": [
        "class FlowerClient(fl.client.NumPyClient):\n",
        "    def __init__(self, model, x_train, y_train, x_val, y_val) -> None:\n",
        "        self.model = model\n",
        "        self.x_train, self.y_train = x_train, y_train\n",
        "        self.x_val, self.y_val = x_val, y_val\n",
        "\n",
        "    def get_parameters(self, config):\n",
        "        return self.model.get_weights()\n",
        "\n",
        "    def fit(self, parameters, config):\n",
        "        self.model.set_weights(parameters)\n",
        "        self.model.fit(self.x_train, self.y_train, epochs=LOCAL_EPOCHS, verbose=2)\n",
        "        return self.model.get_weights(), len(self.x_train), {}\n",
        "\n",
        "    def evaluate(self, parameters, config):\n",
        "        self.model.set_weights(parameters)\n",
        "        loss, acc = self.model.evaluate(self.x_val, self.y_val, verbose=2)\n",
        "        return loss, len(self.x_val), {\"accuracy\": acc}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **FedAvgM** paper uses the same CNN model presented in the following paper:\n",
        "\n",
        "- McMahan, B., Moore, E., Ramage, D., Hampson, S., & y Arcas, B. A. (2017, April). Communication-efficient learning of deep networks from decentralized data. In Artificial intelligence and statistics (pp. 1273-1282). PMLR. ([Link](http://proceedings.mlr.press/v54/mcmahan17a/mcmahan17a.pdf)):\n",
        "\n",
        "As the following excerpt:\n",
        "\n",
        "\"*A CNN with two 5x5 convolution layers (the first with 32 channels, the second with 64, each followed with 2x2 max pooling), a fully connected layer with 512 units and ReLu activation, and a final softmax output layer (1,663,370\n",
        "total parameters)\"*\n",
        "\n",
        "<font color=\"red\">However, this architecture implemented below results in 878,538 parameters.</font>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ged9GZNw5O_A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OavhiopbUWL",
        "outputId": "60e0cb00-6ddb-4c1e-a70a-1df4bae81bf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_2 (Conv2D)           (None, 28, 28, 32)        2432      \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 14, 14, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 10, 10, 64)        51264     \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 5, 5, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 1600)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 512)               819712    \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 10)                5130      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 878,538\n",
            "Trainable params: 878,538\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = keras.Sequential([\n",
        "  keras.layers.Conv2D(32, (5,5), activation='relu', input_shape=input_shape),\n",
        "  keras.layers.MaxPooling2D(2,2),\n",
        "  keras.layers.Conv2D(64, (5,5), activation='relu'),\n",
        "  keras.layers.MaxPooling2D(2,2),\n",
        "  keras.layers.Flatten(),\n",
        "  keras.layers.Dense(512, activation='relu'),\n",
        "  keras.layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Y9SPVuwcKDpv"
      },
      "outputs": [],
      "source": [
        "def client_fn(cid: str) -> fl.client.Client:\n",
        "  partitions = partition(CONCENTRATION)\n",
        "  full_x_train_cid, full_y_train_cid = partitions[int(cid)]\n",
        "  full_y_train_cid = to_categorical(full_y_train_cid, num_classes=num_classes)\n",
        "\n",
        "  # Use 10% of the client's training data for validation\n",
        "  split_idx = math.floor(len(full_x_train_cid) * 0.9)\n",
        "  x_train_cid, y_train_cid = (\n",
        "    full_x_train_cid[:split_idx],\n",
        "    full_y_train_cid[:split_idx],\n",
        "  )\n",
        "  x_val_cid, y_val_cid = full_x_train_cid[split_idx:], full_y_train_cid[split_idx:]\n",
        "\n",
        "\n",
        "  return FlowerClient(model, x_train_cid, y_train_cid, x_val_cid, y_val_cid)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FedAvgM Simulation"
      ],
      "metadata": {
        "id": "mZ1A4Add4ZU5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMYMv4djRnyr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41ce4427-7652-449c-9f0f-ae5587a25ce5"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO flwr 2023-08-22 20:14:13,348 | app.py:146 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)\n",
            "INFO:flwr:Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            ">>> Running with the following parameters:\n",
            ">>>> Concentration (alpha): 100.0\n",
            ">>>> Reporting Fraction (C): 0.05\n",
            ">>>> Local Epochs (E): 1\n",
            "\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-08-22 20:14:17,945\tINFO worker.py:1621 -- Started a local Ray instance.\n",
            "INFO flwr 2023-08-22 20:14:19,629 | app.py:180 | Flower VCE: Ray initialized with resources: {'object_store_memory': 16252830105.0, 'memory': 32505660212.0, 'CPU': 8.0, 'node:172.28.0.12': 1.0, 'node:__internal_head__': 1.0}\n",
            "INFO:flwr:Flower VCE: Ray initialized with resources: {'object_store_memory': 16252830105.0, 'memory': 32505660212.0, 'CPU': 8.0, 'node:172.28.0.12': 1.0, 'node:__internal_head__': 1.0}\n",
            "INFO flwr 2023-08-22 20:14:19,634 | server.py:86 | Initializing global parameters\n",
            "INFO:flwr:Initializing global parameters\n",
            "INFO flwr 2023-08-22 20:14:19,637 | server.py:269 | Using initial parameters provided by strategy\n",
            "INFO:flwr:Using initial parameters provided by strategy\n",
            "INFO flwr 2023-08-22 20:14:19,640 | server.py:88 | Evaluating initial parameters\n",
            "INFO:flwr:Evaluating initial parameters\n",
            "INFO flwr 2023-08-22 20:14:19,643 | server.py:101 | FL starting\n",
            "INFO:flwr:FL starting\n",
            "DEBUG flwr 2023-08-22 20:14:19,645 | server.py:218 | fit_round 1: strategy sampled 5 clients (out of 100)\n",
            "DEBUG:flwr:fit_round 1: strategy sampled 5 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1947540)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1947540)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1947540)\u001b[0m 15/15 - 2s - loss: 2.2919 - accuracy: 0.1067 - 2s/epoch - 118ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-08-22 20:14:39,632 | server.py:232 | fit_round 1 received 5 results and 0 failures\n",
            "DEBUG:flwr:fit_round 1 received 5 results and 0 failures\n",
            "WARNING flwr 2023-08-22 20:14:39,684 | fedavgm.py:214 | No fit_metrics_aggregation_fn provided\n",
            "WARNING:flwr:No fit_metrics_aggregation_fn provided\n",
            "DEBUG flwr 2023-08-22 20:14:39,687 | server.py:168 | evaluate_round 1: strategy sampled 5 clients (out of 100)\n",
            "DEBUG:flwr:evaluate_round 1: strategy sampled 5 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1947546)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 5x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1947545)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1947546)\u001b[0m 15/15 - 2s - loss: 2.3092 - accuracy: 0.1200 - 2s/epoch - 118ms/step\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-08-22 20:14:52,978 | server.py:182 | evaluate_round 1 received 5 results and 0 failures\n",
            "DEBUG:flwr:evaluate_round 1 received 5 results and 0 failures\n",
            "WARNING flwr 2023-08-22 20:14:52,983 | fedavg.py:274 | No evaluate_metrics_aggregation_fn provided\n",
            "WARNING:flwr:No evaluate_metrics_aggregation_fn provided\n",
            "DEBUG flwr 2023-08-22 20:14:52,985 | server.py:218 | fit_round 2: strategy sampled 5 clients (out of 100)\n",
            "DEBUG:flwr:fit_round 2: strategy sampled 5 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1947546)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 5x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1947546)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1947544)\u001b[0m 2/2 - 0s - loss: 2.3019 - accuracy: 0.0800 - 369ms/epoch - 185ms/step\u001b[32m [repeated 5x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-08-22 20:15:08,069 | server.py:232 | fit_round 2 received 5 results and 0 failures\n",
            "DEBUG:flwr:fit_round 2 received 5 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:15:08,109 | server.py:168 | evaluate_round 2: strategy sampled 5 clients (out of 100)\n",
            "DEBUG:flwr:evaluate_round 2: strategy sampled 5 clients (out of 100)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1947543)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1947543)\u001b[0m 15/15 - 2s - loss: 2.1954 - accuracy: 0.1689 - 2s/epoch - 129ms/step\u001b[32m [repeated 5x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1947543)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 5x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1947543)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1947545)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1947543)\u001b[0m 2/2 - 0s - loss: 2.2335 - accuracy: 0.1000 - 432ms/epoch - 216ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-08-22 20:15:22,447 | server.py:182 | evaluate_round 2 received 5 results and 0 failures\n",
            "DEBUG:flwr:evaluate_round 2 received 5 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:15:22,454 | server.py:218 | fit_round 3: strategy sampled 5 clients (out of 100)\n",
            "DEBUG:flwr:fit_round 3: strategy sampled 5 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1947540)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 5x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1947540)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1947544)\u001b[0m 2/2 - 0s - loss: 2.2077 - accuracy: 0.1800 - 290ms/epoch - 145ms/step\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-08-22 20:15:37,471 | server.py:232 | fit_round 3 received 5 results and 0 failures\n",
            "DEBUG:flwr:fit_round 3 received 5 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:15:37,517 | server.py:168 | evaluate_round 3: strategy sampled 5 clients (out of 100)\n",
            "DEBUG:flwr:evaluate_round 3: strategy sampled 5 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1947543)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 5x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1947540)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1947543)\u001b[0m 15/15 - 2s - loss: 2.1085 - accuracy: 0.2444 - 2s/epoch - 138ms/step\u001b[32m [repeated 5x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-08-22 20:15:49,953 | server.py:182 | evaluate_round 3 received 5 results and 0 failures\n",
            "DEBUG:flwr:evaluate_round 3 received 5 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:15:49,956 | server.py:218 | fit_round 4: strategy sampled 5 clients (out of 100)\n",
            "DEBUG:flwr:fit_round 4: strategy sampled 5 clients (out of 100)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1947545)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1947545)\u001b[0m 2/2 - 0s - loss: 3.3983 - accuracy: 0.2400 - 292ms/epoch - 146ms/step\u001b[32m [repeated 5x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1947545)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 5x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1947545)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1947543)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1947545)\u001b[0m 15/15 - 6s - loss: 2.3558 - accuracy: 0.1933 - 6s/epoch - 421ms/step\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1947546)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-08-22 20:16:11,154 | server.py:232 | fit_round 4 received 5 results and 0 failures\n",
            "DEBUG:flwr:fit_round 4 received 5 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:16:11,367 | server.py:168 | evaluate_round 4: strategy sampled 5 clients (out of 100)\n",
            "DEBUG:flwr:evaluate_round 4: strategy sampled 5 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1947546)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 5x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1947546)\u001b[0m 15/15 - 8s - loss: 2.1671 - accuracy: 0.2622 - 8s/epoch - 545ms/step\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1947545)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1947546)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-08-22 20:16:45,985 | server.py:182 | evaluate_round 4 received 5 results and 0 failures\n",
            "DEBUG:flwr:evaluate_round 4 received 5 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:16:45,995 | server.py:218 | fit_round 5: strategy sampled 5 clients (out of 100)\n",
            "DEBUG:flwr:fit_round 5: strategy sampled 5 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1947540)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 5x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1947540)\u001b[0m 2/2 - 1s - loss: 3.9458 - accuracy: 0.1200 - 937ms/epoch - 468ms/step\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1947543)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-08-22 20:17:12,304 | server.py:232 | fit_round 5 received 5 results and 0 failures\n",
            "DEBUG:flwr:fit_round 5 received 5 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:17:12,345 | server.py:168 | evaluate_round 5: strategy sampled 5 clients (out of 100)\n",
            "DEBUG:flwr:evaluate_round 5: strategy sampled 5 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1947540)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 5x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1947545)\u001b[0m 15/15 - 2s - loss: 2.3879 - accuracy: 0.2311 - 2s/epoch - 144ms/step\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1947545)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 5x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1947545)\u001b[0m WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_test_function.<locals>.test_function at 0x78103d66ab90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1947545)\u001b[0m WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_test_function.<locals>.test_function at 0x78103d66ab90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "DEBUG flwr 2023-08-22 20:17:27,415 | server.py:182 | evaluate_round 5 received 5 results and 0 failures\n",
            "DEBUG:flwr:evaluate_round 5 received 5 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:17:27,418 | server.py:218 | fit_round 6: strategy sampled 5 clients (out of 100)\n",
            "DEBUG:flwr:fit_round 6: strategy sampled 5 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1947540)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1947544)\u001b[0m WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_test_function.<locals>.test_function at 0x7f854c481870> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\u001b[32m [repeated 8x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1947544)\u001b[0m 2/2 - 0s - loss: 3.3416 - accuracy: 0.1800 - 303ms/epoch - 151ms/step\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1947540)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 5x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-08-22 20:17:44,254 | server.py:232 | fit_round 6 received 5 results and 0 failures\n",
            "DEBUG:flwr:fit_round 6 received 5 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:17:44,292 | server.py:168 | evaluate_round 6: strategy sampled 5 clients (out of 100)\n",
            "DEBUG:flwr:evaluate_round 6: strategy sampled 5 clients (out of 100)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1947546)\u001b[0m 15/15 - 2s - loss: 2.2308 - accuracy: 0.2156 - 2s/epoch - 160ms/step\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1947544)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1947546)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 5x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1947546)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1947545)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1947545)\u001b[0m 2/2 - 0s - loss: 2.0968 - accuracy: 0.2200 - 310ms/epoch - 155ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1947545)\u001b[0m WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x78103d78d510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1947545)\u001b[0m WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x78103d78d510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "DEBUG flwr 2023-08-22 20:17:57,131 | server.py:182 | evaluate_round 6 received 5 results and 0 failures\n",
            "DEBUG:flwr:evaluate_round 6 received 5 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:17:57,135 | server.py:218 | fit_round 7: strategy sampled 5 clients (out of 100)\n",
            "DEBUG:flwr:fit_round 7: strategy sampled 5 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1947540)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1947544)\u001b[0m WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f857846acb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\u001b[32m [repeated 8x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1947543)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1947544)\u001b[0m 2/2 - 0s - loss: 2.0076 - accuracy: 0.2200 - 375ms/epoch - 188ms/step\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-08-22 20:18:11,306 | server.py:232 | fit_round 7 received 5 results and 0 failures\n",
            "DEBUG:flwr:fit_round 7 received 5 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:18:11,344 | server.py:168 | evaluate_round 7: strategy sampled 5 clients (out of 100)\n",
            "DEBUG:flwr:evaluate_round 7: strategy sampled 5 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1947540)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 5x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1947546)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1947545)\u001b[0m 15/15 - 2s - loss: 2.0490 - accuracy: 0.2578 - 2s/epoch - 152ms/step\u001b[32m [repeated 5x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-08-22 20:18:23,376 | server.py:182 | evaluate_round 7 received 5 results and 0 failures\n",
            "DEBUG:flwr:evaluate_round 7 received 5 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:18:23,378 | server.py:218 | fit_round 8: strategy sampled 5 clients (out of 100)\n",
            "DEBUG:flwr:fit_round 8: strategy sampled 5 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1947543)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 5x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1947540)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1947543)\u001b[0m 2/2 - 0s - loss: 2.1187 - accuracy: 0.2000 - 287ms/epoch - 144ms/step\u001b[32m [repeated 5x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-08-22 20:18:37,708 | server.py:232 | fit_round 8 received 5 results and 0 failures\n",
            "DEBUG:flwr:fit_round 8 received 5 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:18:37,747 | server.py:168 | evaluate_round 8: strategy sampled 5 clients (out of 100)\n",
            "DEBUG:flwr:evaluate_round 8: strategy sampled 5 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1947546)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 5x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1947543)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1947544)\u001b[0m 15/15 - 2s - loss: 1.8971 - accuracy: 0.3133 - 2s/epoch - 132ms/step\u001b[32m [repeated 5x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-08-22 20:18:51,088 | server.py:182 | evaluate_round 8 received 5 results and 0 failures\n",
            "DEBUG:flwr:evaluate_round 8 received 5 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:18:51,091 | server.py:218 | fit_round 9: strategy sampled 5 clients (out of 100)\n",
            "DEBUG:flwr:fit_round 9: strategy sampled 5 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1947545)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 5x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1947544)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1947545)\u001b[0m 2/2 - 0s - loss: 1.9589 - accuracy: 0.3600 - 271ms/epoch - 135ms/step\u001b[32m [repeated 5x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-08-22 20:19:05,518 | server.py:232 | fit_round 9 received 5 results and 0 failures\n",
            "DEBUG:flwr:fit_round 9 received 5 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:19:05,557 | server.py:168 | evaluate_round 9: strategy sampled 5 clients (out of 100)\n",
            "DEBUG:flwr:evaluate_round 9: strategy sampled 5 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1947546)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 5x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1947545)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1947540)\u001b[0m 15/15 - 2s - loss: 2.0222 - accuracy: 0.2778 - 2s/epoch - 139ms/step\u001b[32m [repeated 5x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-08-22 20:19:17,551 | server.py:182 | evaluate_round 9 received 5 results and 0 failures\n",
            "DEBUG:flwr:evaluate_round 9 received 5 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:19:17,554 | server.py:218 | fit_round 10: strategy sampled 5 clients (out of 100)\n",
            "DEBUG:flwr:fit_round 10: strategy sampled 5 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1947543)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 5x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1947544)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1947544)\u001b[0m 2/2 - 0s - loss: 1.9147 - accuracy: 0.3000 - 432ms/epoch - 216ms/step\u001b[32m [repeated 5x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-08-22 20:19:31,568 | server.py:232 | fit_round 10 received 5 results and 0 failures\n",
            "DEBUG:flwr:fit_round 10 received 5 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:19:31,605 | server.py:168 | evaluate_round 10: strategy sampled 5 clients (out of 100)\n",
            "DEBUG:flwr:evaluate_round 10: strategy sampled 5 clients (out of 100)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1947545)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1947545)\u001b[0m 15/15 - 2s - loss: 2.0097 - accuracy: 0.2800 - 2s/epoch - 122ms/step\u001b[32m [repeated 5x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1947545)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 5x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1947540)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1947545)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1947540)\u001b[0m 2/2 - 0s - loss: 3.5843 - accuracy: 0.1600 - 345ms/epoch - 173ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-08-22 20:19:44,286 | server.py:182 | evaluate_round 10 received 5 results and 0 failures\n",
            "DEBUG:flwr:evaluate_round 10 received 5 results and 0 failures\n",
            "INFO flwr 2023-08-22 20:19:44,289 | server.py:147 | FL finished in 324.6440631860005\n",
            "INFO:flwr:FL finished in 324.6440631860005\n",
            "INFO flwr 2023-08-22 20:19:44,292 | app.py:218 | app_fit: losses_distributed [(1, 2.2882763385772704), (2, 2.1375440120697022), (3, 2.933402156829834), (4, 4.019338941574096), (5, 3.5648717403411867), (6, 2.0449785947799684), (7, 2.090142321586609), (8, 1.9163742780685424), (9, 2.081928014755249), (10, 3.3417354106903074)]\n",
            "INFO:flwr:app_fit: losses_distributed [(1, 2.2882763385772704), (2, 2.1375440120697022), (3, 2.933402156829834), (4, 4.019338941574096), (5, 3.5648717403411867), (6, 2.0449785947799684), (7, 2.090142321586609), (8, 1.9163742780685424), (9, 2.081928014755249), (10, 3.3417354106903074)]\n",
            "INFO flwr 2023-08-22 20:19:44,295 | app.py:219 | app_fit: metrics_distributed_fit {}\n",
            "INFO:flwr:app_fit: metrics_distributed_fit {}\n",
            "INFO flwr 2023-08-22 20:19:44,299 | app.py:220 | app_fit: metrics_distributed {}\n",
            "INFO:flwr:app_fit: metrics_distributed {}\n",
            "INFO flwr 2023-08-22 20:19:44,308 | app.py:221 | app_fit: losses_centralized []\n",
            "INFO:flwr:app_fit: losses_centralized []\n",
            "INFO flwr 2023-08-22 20:19:44,311 | app.py:222 | app_fit: metrics_centralized {}\n",
            "INFO:flwr:app_fit: metrics_centralized {}\n",
            "INFO flwr 2023-08-22 20:19:44,330 | app.py:146 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)\n",
            "INFO:flwr:Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1947544)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 4x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            ">>> Running with the following parameters:\n",
            ">>>> Concentration (alpha): 100.0\n",
            ">>>> Reporting Fraction (C): 0.05\n",
            ">>>> Local Epochs (E): 5\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1947544)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1947546)\u001b[0m 2/2 - 0s - loss: 3.1723 - accuracy: 0.1400 - 376ms/epoch - 188ms/step\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-08-22 20:19:48,954\tINFO worker.py:1621 -- Started a local Ray instance.\n",
            "INFO flwr 2023-08-22 20:19:50,621 | app.py:180 | Flower VCE: Ray initialized with resources: {'CPU': 8.0, 'node:172.28.0.12': 1.0, 'node:__internal_head__': 1.0, 'memory': 32504812340.0, 'object_store_memory': 16252406169.0}\n",
            "INFO:flwr:Flower VCE: Ray initialized with resources: {'CPU': 8.0, 'node:172.28.0.12': 1.0, 'node:__internal_head__': 1.0, 'memory': 32504812340.0, 'object_store_memory': 16252406169.0}\n",
            "INFO flwr 2023-08-22 20:19:50,624 | server.py:86 | Initializing global parameters\n",
            "INFO:flwr:Initializing global parameters\n",
            "INFO flwr 2023-08-22 20:19:50,627 | server.py:269 | Using initial parameters provided by strategy\n",
            "INFO:flwr:Using initial parameters provided by strategy\n",
            "INFO flwr 2023-08-22 20:19:50,630 | server.py:88 | Evaluating initial parameters\n",
            "INFO:flwr:Evaluating initial parameters\n",
            "INFO flwr 2023-08-22 20:19:50,632 | server.py:101 | FL starting\n",
            "INFO:flwr:FL starting\n",
            "DEBUG flwr 2023-08-22 20:19:50,635 | server.py:218 | fit_round 1: strategy sampled 5 clients (out of 100)\n",
            "DEBUG:flwr:fit_round 1: strategy sampled 5 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950589)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950589)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950589)\u001b[0m Epoch 1/5\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950586)\u001b[0m 15/15 - 2s - loss: 2.3094 - accuracy: 0.1400 - 2s/epoch - 146ms/step\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950588)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950589)\u001b[0m Epoch 4/5\u001b[32m [repeated 14x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950588)\u001b[0m 15/15 - 1s - loss: 2.0379 - accuracy: 0.2867 - 1s/epoch - 88ms/step\u001b[32m [repeated 17x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-08-22 20:20:15,693 | server.py:232 | fit_round 1 received 5 results and 0 failures\n",
            "DEBUG:flwr:fit_round 1 received 5 results and 0 failures\n",
            "WARNING flwr 2023-08-22 20:20:15,738 | fedavgm.py:214 | No fit_metrics_aggregation_fn provided\n",
            "WARNING:flwr:No fit_metrics_aggregation_fn provided\n",
            "DEBUG flwr 2023-08-22 20:20:15,741 | server.py:168 | evaluate_round 1: strategy sampled 5 clients (out of 100)\n",
            "DEBUG:flwr:evaluate_round 1: strategy sampled 5 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1950588)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 5x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1950588)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950588)\u001b[0m Epoch 5/5\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950588)\u001b[0m 15/15 - 0s - loss: 1.7202 - accuracy: 0.3578 - 393ms/epoch - 26ms/step\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1950587)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-08-22 20:20:28,569 | server.py:182 | evaluate_round 1 received 5 results and 0 failures\n",
            "DEBUG:flwr:evaluate_round 1 received 5 results and 0 failures\n",
            "WARNING flwr 2023-08-22 20:20:28,572 | fedavg.py:274 | No evaluate_metrics_aggregation_fn provided\n",
            "WARNING:flwr:No evaluate_metrics_aggregation_fn provided\n",
            "DEBUG flwr 2023-08-22 20:20:28,575 | server.py:218 | fit_round 2: strategy sampled 5 clients (out of 100)\n",
            "DEBUG:flwr:fit_round 2: strategy sampled 5 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950589)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 5x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1950589)\u001b[0m 2/2 - 0s - loss: 2.0159 - accuracy: 0.3000 - 327ms/epoch - 163ms/step\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950587)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950587)\u001b[0m Epoch 1/5\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950590)\u001b[0m 15/15 - 1s - loss: 1.6439 - accuracy: 0.3978 - 1s/epoch - 90ms/step\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950588)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950590)\u001b[0m Epoch 4/5\u001b[32m [repeated 16x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-08-22 20:20:48,215 | server.py:232 | fit_round 2 received 5 results and 0 failures\n",
            "DEBUG:flwr:fit_round 2 received 5 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:20:48,257 | server.py:168 | evaluate_round 2: strategy sampled 5 clients (out of 100)\n",
            "DEBUG:flwr:evaluate_round 2: strategy sampled 5 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1950588)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 5x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950588)\u001b[0m 15/15 - 0s - loss: 1.3180 - accuracy: 0.5489 - 434ms/epoch - 29ms/step\u001b[32m [repeated 13x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1950588)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950588)\u001b[0m Epoch 5/5\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1950589)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-08-22 20:21:01,810 | server.py:182 | evaluate_round 2 received 5 results and 0 failures\n",
            "DEBUG:flwr:evaluate_round 2 received 5 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:21:01,814 | server.py:218 | fit_round 3: strategy sampled 5 clients (out of 100)\n",
            "DEBUG:flwr:fit_round 3: strategy sampled 5 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950587)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 5x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1950587)\u001b[0m 2/2 - 0s - loss: 3.2817 - accuracy: 0.2800 - 394ms/epoch - 197ms/step\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950586)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950586)\u001b[0m Epoch 1/5\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950590)\u001b[0m 15/15 - 2s - loss: 1.5489 - accuracy: 0.4489 - 2s/epoch - 101ms/step\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950587)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950590)\u001b[0m Epoch 4/5\u001b[32m [repeated 15x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-08-22 20:21:22,310 | server.py:232 | fit_round 3 received 5 results and 0 failures\n",
            "DEBUG:flwr:fit_round 3 received 5 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:21:22,348 | server.py:168 | evaluate_round 3: strategy sampled 5 clients (out of 100)\n",
            "DEBUG:flwr:evaluate_round 3: strategy sampled 5 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1950589)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 5x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950589)\u001b[0m 15/15 - 1s - loss: 1.3346 - accuracy: 0.5422 - 911ms/epoch - 61ms/step\u001b[32m [repeated 14x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1950589)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950589)\u001b[0m Epoch 5/5\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1950590)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-08-22 20:21:34,406 | server.py:182 | evaluate_round 3 received 5 results and 0 failures\n",
            "DEBUG:flwr:evaluate_round 3 received 5 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:21:34,411 | server.py:218 | fit_round 4: strategy sampled 5 clients (out of 100)\n",
            "DEBUG:flwr:fit_round 4: strategy sampled 5 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950586)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 5x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1950588)\u001b[0m 2/2 - 0s - loss: 3.7335 - accuracy: 0.4400 - 326ms/epoch - 163ms/step\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950588)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950588)\u001b[0m Epoch 1/5\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950588)\u001b[0m 15/15 - 1s - loss: 1.2392 - accuracy: 0.5667 - 1s/epoch - 97ms/step\u001b[32m [repeated 14x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950586)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950588)\u001b[0m Epoch 5/5\u001b[32m [repeated 18x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-08-22 20:21:53,388 | server.py:232 | fit_round 4 received 5 results and 0 failures\n",
            "DEBUG:flwr:fit_round 4 received 5 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:21:53,428 | server.py:168 | evaluate_round 4: strategy sampled 5 clients (out of 100)\n",
            "DEBUG:flwr:evaluate_round 4: strategy sampled 5 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1950586)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 5x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950590)\u001b[0m 15/15 - 1s - loss: 1.2393 - accuracy: 0.5600 - 610ms/epoch - 41ms/step\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1950590)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950590)\u001b[0m Epoch 5/5\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1950589)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-08-22 20:22:06,201 | server.py:182 | evaluate_round 4 received 5 results and 0 failures\n",
            "DEBUG:flwr:evaluate_round 4 received 5 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:22:06,205 | server.py:218 | fit_round 5: strategy sampled 5 clients (out of 100)\n",
            "DEBUG:flwr:fit_round 5: strategy sampled 5 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950586)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 5x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1950588)\u001b[0m 2/2 - 0s - loss: 2.6120 - accuracy: 0.3600 - 374ms/epoch - 187ms/step\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950589)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950589)\u001b[0m Epoch 1/5\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950586)\u001b[0m 15/15 - 1s - loss: 1.2617 - accuracy: 0.5600 - 1s/epoch - 84ms/step\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950588)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950588)\u001b[0m Epoch 4/5\u001b[32m [repeated 17x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-08-22 20:22:25,821 | server.py:232 | fit_round 5 received 5 results and 0 failures\n",
            "DEBUG:flwr:fit_round 5 received 5 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:22:25,867 | server.py:168 | evaluate_round 5: strategy sampled 5 clients (out of 100)\n",
            "DEBUG:flwr:evaluate_round 5: strategy sampled 5 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1950586)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 5x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950587)\u001b[0m 15/15 - 1s - loss: 1.1130 - accuracy: 0.6333 - 962ms/epoch - 64ms/step\u001b[32m [repeated 13x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1950589)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950587)\u001b[0m Epoch 5/5\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1950588)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1950588)\u001b[0m WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_test_function.<locals>.test_function at 0x7ce7485bfbe0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1950588)\u001b[0m WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_test_function.<locals>.test_function at 0x7ce7485bfbe0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "DEBUG flwr 2023-08-22 20:22:38,036 | server.py:182 | evaluate_round 5 received 5 results and 0 failures\n",
            "DEBUG:flwr:evaluate_round 5 received 5 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:22:38,040 | server.py:218 | fit_round 6: strategy sampled 5 clients (out of 100)\n",
            "DEBUG:flwr:fit_round 6: strategy sampled 5 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950586)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1950587)\u001b[0m WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_test_function.<locals>.test_function at 0x7ef5cff68f70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\u001b[32m [repeated 8x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1950587)\u001b[0m 2/2 - 0s - loss: 2.2889 - accuracy: 0.4000 - 375ms/epoch - 187ms/step\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950590)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950590)\u001b[0m Epoch 1/5\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950590)\u001b[0m 15/15 - 1s - loss: 1.2626 - accuracy: 0.5556 - 1s/epoch - 86ms/step\u001b[32m [repeated 13x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950589)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950586)\u001b[0m Epoch 4/5\u001b[32m [repeated 16x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-08-22 20:22:56,858 | server.py:232 | fit_round 6 received 5 results and 0 failures\n",
            "DEBUG:flwr:fit_round 6 received 5 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:22:56,899 | server.py:168 | evaluate_round 6: strategy sampled 5 clients (out of 100)\n",
            "DEBUG:flwr:evaluate_round 6: strategy sampled 5 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1950588)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 5x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950588)\u001b[0m 15/15 - 1s - loss: 0.7639 - accuracy: 0.7556 - 995ms/epoch - 66ms/step\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1950590)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950589)\u001b[0m Epoch 5/5\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1950586)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1950590)\u001b[0m WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7a2fd0a6a0e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1950590)\u001b[0m WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7a2fd0a6a0e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "DEBUG flwr 2023-08-22 20:23:09,591 | server.py:182 | evaluate_round 6 received 5 results and 0 failures\n",
            "DEBUG:flwr:evaluate_round 6 received 5 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:23:09,594 | server.py:218 | fit_round 7: strategy sampled 5 clients (out of 100)\n",
            "DEBUG:flwr:fit_round 7: strategy sampled 5 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1950587)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1950587)\u001b[0m WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7ef5a851de10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950589)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950587)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1950587)\u001b[0m 2/2 - 0s - loss: 2.0730 - accuracy: 0.3000 - 274ms/epoch - 137ms/step\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950587)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950587)\u001b[0m Epoch 1/5\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950588)\u001b[0m 15/15 - 1s - loss: 1.1591 - accuracy: 0.6067 - 1s/epoch - 89ms/step\u001b[32m [repeated 13x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950590)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950588)\u001b[0m Epoch 4/5\u001b[32m [repeated 17x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-08-22 20:23:29,374 | server.py:232 | fit_round 7 received 5 results and 0 failures\n",
            "DEBUG:flwr:fit_round 7 received 5 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:23:29,412 | server.py:168 | evaluate_round 7: strategy sampled 5 clients (out of 100)\n",
            "DEBUG:flwr:evaluate_round 7: strategy sampled 5 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1950589)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 4x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950590)\u001b[0m 15/15 - 1s - loss: 0.6702 - accuracy: 0.7822 - 985ms/epoch - 66ms/step\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1950586)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950589)\u001b[0m Epoch 5/5\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1950587)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-08-22 20:23:41,424 | server.py:182 | evaluate_round 7 received 5 results and 0 failures\n",
            "DEBUG:flwr:evaluate_round 7 received 5 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:23:41,430 | server.py:218 | fit_round 8: strategy sampled 5 clients (out of 100)\n",
            "DEBUG:flwr:fit_round 8: strategy sampled 5 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950590)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 5x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1950588)\u001b[0m 2/2 - 0s - loss: 1.7451 - accuracy: 0.5000 - 296ms/epoch - 148ms/step\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950588)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950588)\u001b[0m Epoch 1/5\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950590)\u001b[0m 15/15 - 1s - loss: 0.9291 - accuracy: 0.6889 - 1s/epoch - 92ms/step\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950589)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950589)\u001b[0m Epoch 3/5\u001b[32m [repeated 15x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-08-22 20:24:01,145 | server.py:232 | fit_round 8 received 5 results and 0 failures\n",
            "DEBUG:flwr:fit_round 8 received 5 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:24:01,186 | server.py:168 | evaluate_round 8: strategy sampled 5 clients (out of 100)\n",
            "DEBUG:flwr:evaluate_round 8: strategy sampled 5 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1950587)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 5x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950589)\u001b[0m 15/15 - 1s - loss: 0.6246 - accuracy: 0.7911 - 705ms/epoch - 47ms/step\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1950587)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950589)\u001b[0m Epoch 5/5\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1950590)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-08-22 20:24:13,795 | server.py:182 | evaluate_round 8 received 5 results and 0 failures\n",
            "DEBUG:flwr:evaluate_round 8 received 5 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:24:13,801 | server.py:218 | fit_round 9: strategy sampled 5 clients (out of 100)\n",
            "DEBUG:flwr:fit_round 9: strategy sampled 5 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950588)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 5x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1950588)\u001b[0m 2/2 - 0s - loss: 2.4555 - accuracy: 0.5200 - 471ms/epoch - 236ms/step\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950590)\u001b[0m Epoch 1/5\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950590)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950588)\u001b[0m Epoch 1/5\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950590)\u001b[0m 15/15 - 1s - loss: 1.0412 - accuracy: 0.6422 - 1s/epoch - 91ms/step\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950586)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950590)\u001b[0m Epoch 4/5\u001b[32m [repeated 14x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-08-22 20:24:34,163 | server.py:232 | fit_round 9 received 5 results and 0 failures\n",
            "DEBUG:flwr:fit_round 9 received 5 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:24:34,204 | server.py:168 | evaluate_round 9: strategy sampled 5 clients (out of 100)\n",
            "DEBUG:flwr:evaluate_round 9: strategy sampled 5 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1950586)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 5x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950587)\u001b[0m 15/15 - 1s - loss: 0.5217 - accuracy: 0.8422 - 855ms/epoch - 57ms/step\u001b[32m [repeated 14x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1950586)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950587)\u001b[0m Epoch 5/5\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1950587)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-08-22 20:24:46,637 | server.py:182 | evaluate_round 9 received 5 results and 0 failures\n",
            "DEBUG:flwr:evaluate_round 9 received 5 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:24:46,640 | server.py:218 | fit_round 10: strategy sampled 5 clients (out of 100)\n",
            "DEBUG:flwr:fit_round 10: strategy sampled 5 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950586)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 5x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1950590)\u001b[0m 2/2 - 0s - loss: 2.5443 - accuracy: 0.4800 - 331ms/epoch - 165ms/step\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950586)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950586)\u001b[0m Epoch 1/5\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950590)\u001b[0m 15/15 - 1s - loss: 1.4940 - accuracy: 0.5267 - 1s/epoch - 98ms/step\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950587)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950588)\u001b[0m Epoch 4/5\u001b[32m [repeated 16x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-08-22 20:25:05,915 | server.py:232 | fit_round 10 received 5 results and 0 failures\n",
            "DEBUG:flwr:fit_round 10 received 5 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:25:05,957 | server.py:168 | evaluate_round 10: strategy sampled 5 clients (out of 100)\n",
            "DEBUG:flwr:evaluate_round 10: strategy sampled 5 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1950590)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 5x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950587)\u001b[0m 15/15 - 1s - loss: 0.8038 - accuracy: 0.7111 - 512ms/epoch - 34ms/step\u001b[32m [repeated 14x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1950589)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1950587)\u001b[0m Epoch 5/5\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1950586)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-08-22 20:25:18,644 | server.py:182 | evaluate_round 10 received 5 results and 0 failures\n",
            "DEBUG:flwr:evaluate_round 10 received 5 results and 0 failures\n",
            "INFO flwr 2023-08-22 20:25:18,649 | server.py:147 | FL finished in 328.01470316199993\n",
            "INFO:flwr:FL finished in 328.01470316199993\n",
            "INFO flwr 2023-08-22 20:25:18,652 | app.py:218 | app_fit: losses_distributed [(1, 2.105103778839111), (2, 3.365287113189697), (3, 3.3317336082458495), (4, 2.2878280878067017), (5, 2.1813233613967897), (6, 1.890648078918457), (7, 1.938372015953064), (8, 2.3311931610107424), (9, 2.324869704246521), (10, 2.414632868766785)]\n",
            "INFO:flwr:app_fit: losses_distributed [(1, 2.105103778839111), (2, 3.365287113189697), (3, 3.3317336082458495), (4, 2.2878280878067017), (5, 2.1813233613967897), (6, 1.890648078918457), (7, 1.938372015953064), (8, 2.3311931610107424), (9, 2.324869704246521), (10, 2.414632868766785)]\n",
            "INFO flwr 2023-08-22 20:25:18,654 | app.py:219 | app_fit: metrics_distributed_fit {}\n",
            "INFO:flwr:app_fit: metrics_distributed_fit {}\n",
            "INFO flwr 2023-08-22 20:25:18,659 | app.py:220 | app_fit: metrics_distributed {}\n",
            "INFO:flwr:app_fit: metrics_distributed {}\n",
            "INFO flwr 2023-08-22 20:25:18,662 | app.py:221 | app_fit: losses_centralized []\n",
            "INFO:flwr:app_fit: losses_centralized []\n",
            "INFO flwr 2023-08-22 20:25:18,666 | app.py:222 | app_fit: metrics_centralized {}\n",
            "INFO:flwr:app_fit: metrics_centralized {}\n",
            "INFO flwr 2023-08-22 20:25:18,688 | app.py:146 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)\n",
            "INFO:flwr:Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1950586)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 4x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            ">>> Running with the following parameters:\n",
            ">>>> Concentration (alpha): 100.0\n",
            ">>>> Reporting Fraction (C): 0.1\n",
            ">>>> Local Epochs (E): 1\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1950588)\u001b[0m 2/2 - 0s - loss: 1.9220 - accuracy: 0.4400 - 331ms/epoch - 166ms/step\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1950588)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-08-22 20:25:23,459\tINFO worker.py:1621 -- Started a local Ray instance.\n",
            "INFO flwr 2023-08-22 20:25:25,080 | app.py:180 | Flower VCE: Ray initialized with resources: {'object_store_memory': 16251928166.0, 'memory': 32503856334.0, 'CPU': 8.0, 'node:172.28.0.12': 1.0, 'node:__internal_head__': 1.0}\n",
            "INFO:flwr:Flower VCE: Ray initialized with resources: {'object_store_memory': 16251928166.0, 'memory': 32503856334.0, 'CPU': 8.0, 'node:172.28.0.12': 1.0, 'node:__internal_head__': 1.0}\n",
            "INFO flwr 2023-08-22 20:25:25,083 | server.py:86 | Initializing global parameters\n",
            "INFO:flwr:Initializing global parameters\n",
            "INFO flwr 2023-08-22 20:25:25,085 | server.py:269 | Using initial parameters provided by strategy\n",
            "INFO:flwr:Using initial parameters provided by strategy\n",
            "INFO flwr 2023-08-22 20:25:25,093 | server.py:88 | Evaluating initial parameters\n",
            "INFO:flwr:Evaluating initial parameters\n",
            "INFO flwr 2023-08-22 20:25:25,095 | server.py:101 | FL starting\n",
            "INFO:flwr:FL starting\n",
            "DEBUG flwr 2023-08-22 20:25:25,097 | server.py:218 | fit_round 1: strategy sampled 10 clients (out of 100)\n",
            "DEBUG:flwr:fit_round 1: strategy sampled 10 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1953646)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1953648)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1953648)\u001b[0m 15/15 - 4s - loss: 2.2717 - accuracy: 0.1600 - 4s/epoch - 250ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1953648)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 8x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1953648)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 8x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-08-22 20:26:02,224 | server.py:232 | fit_round 1 received 10 results and 0 failures\n",
            "DEBUG:flwr:fit_round 1 received 10 results and 0 failures\n",
            "WARNING flwr 2023-08-22 20:26:02,310 | fedavgm.py:214 | No fit_metrics_aggregation_fn provided\n",
            "WARNING:flwr:No fit_metrics_aggregation_fn provided\n",
            "DEBUG flwr 2023-08-22 20:26:02,313 | server.py:168 | evaluate_round 1: strategy sampled 10 clients (out of 100)\n",
            "DEBUG:flwr:evaluate_round 1: strategy sampled 10 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1953646)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 2x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1953646)\u001b[0m 15/15 - 1s - loss: 2.3171 - accuracy: 0.0889 - 1s/epoch - 88ms/step\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1953650)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-08-22 20:26:27,698 | server.py:182 | evaluate_round 1 received 10 results and 0 failures\n",
            "DEBUG:flwr:evaluate_round 1 received 10 results and 0 failures\n",
            "WARNING flwr 2023-08-22 20:26:27,701 | fedavg.py:274 | No evaluate_metrics_aggregation_fn provided\n",
            "WARNING:flwr:No evaluate_metrics_aggregation_fn provided\n",
            "DEBUG flwr 2023-08-22 20:26:27,703 | server.py:218 | fit_round 2: strategy sampled 10 clients (out of 100)\n",
            "DEBUG:flwr:fit_round 2: strategy sampled 10 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1953650)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 10x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1953647)\u001b[0m 2/2 - 0s - loss: 2.2718 - accuracy: 0.1800 - 275ms/epoch - 138ms/step\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1953650)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 10x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1953647)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 8x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1953645)\u001b[0m 15/15 - 4s - loss: 2.1581 - accuracy: 0.1733 - 4s/epoch - 238ms/step\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1953647)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 8x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-08-22 20:26:58,575 | server.py:232 | fit_round 2 received 10 results and 0 failures\n",
            "DEBUG:flwr:fit_round 2 received 10 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:26:58,642 | server.py:168 | evaluate_round 2: strategy sampled 10 clients (out of 100)\n",
            "DEBUG:flwr:evaluate_round 2: strategy sampled 10 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1953643)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 2x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1953643)\u001b[0m 15/15 - 1s - loss: 2.1592 - accuracy: 0.2044 - 1s/epoch - 89ms/step\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1953643)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1953643)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 8x across cluster]\u001b[0m\n",
            "DEBUG flwr 2023-08-22 20:28:09,610 | server.py:182 | evaluate_round 2 received 10 results and 0 failures\n",
            "DEBUG:flwr:evaluate_round 2 received 10 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:28:09,614 | server.py:218 | fit_round 3: strategy sampled 10 clients (out of 100)\n",
            "DEBUG:flwr:fit_round 3: strategy sampled 10 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1953649)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 2x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1953649)\u001b[0m 2/2 - 0s - loss: 2.1617 - accuracy: 0.1600 - 284ms/epoch - 142ms/step\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1953649)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 10x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1953649)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 8x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1953643)\u001b[0m 15/15 - 4s - loss: 2.1160 - accuracy: 0.2244 - 4s/epoch - 279ms/step\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1953647)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 7x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-08-22 20:28:45,100 | server.py:232 | fit_round 3 received 10 results and 0 failures\n",
            "DEBUG:flwr:fit_round 3 received 10 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:28:45,164 | server.py:168 | evaluate_round 3: strategy sampled 10 clients (out of 100)\n",
            "DEBUG:flwr:evaluate_round 3: strategy sampled 10 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1953644)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 2x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1953644)\u001b[0m 15/15 - 2s - loss: 2.0472 - accuracy: 0.2378 - 2s/epoch - 121ms/step\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1953644)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-08-22 20:29:10,356 | server.py:182 | evaluate_round 3 received 10 results and 0 failures\n",
            "DEBUG:flwr:evaluate_round 3 received 10 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:29:10,359 | server.py:218 | fit_round 4: strategy sampled 10 clients (out of 100)\n",
            "DEBUG:flwr:fit_round 4: strategy sampled 10 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1953649)\u001b[0m WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_test_function.<locals>.test_function at 0x7836d4309a20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1953649)\u001b[0m WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_test_function.<locals>.test_function at 0x7836d4309a20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1953649)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 9x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1953649)\u001b[0m 2/2 - 0s - loss: 2.2942 - accuracy: 0.3200 - 347ms/epoch - 174ms/step\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1953644)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 9x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1953649)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1953645)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1953644)\u001b[0m 2/2 - 0s - loss: 2.0335 - accuracy: 0.2400 - 289ms/epoch - 144ms/step\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1953649)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1953645)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1953649)\u001b[0m 15/15 - 3s - loss: 2.0394 - accuracy: 0.2689 - 3s/epoch - 188ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1953649)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 7x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1953645)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 7x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-08-22 20:29:38,664 | server.py:232 | fit_round 4 received 10 results and 0 failures\n",
            "DEBUG:flwr:fit_round 4 received 10 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:29:38,732 | server.py:168 | evaluate_round 4: strategy sampled 10 clients (out of 100)\n",
            "DEBUG:flwr:evaluate_round 4: strategy sampled 10 clients (out of 100)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1953645)\u001b[0m 15/15 - 1s - loss: 2.1145 - accuracy: 0.2844 - 1s/epoch - 92ms/step\u001b[32m [repeated 9x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1953643)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 2x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1953645)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1953643)\u001b[0m 2/2 - 0s - loss: 3.9642 - accuracy: 0.2400 - 474ms/epoch - 237ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1953643)\u001b[0m WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_test_function.<locals>.test_function at 0x7a80ae4ade10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1953643)\u001b[0m WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_test_function.<locals>.test_function at 0x7a80ae4ade10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1953645)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 9x across cluster]\u001b[0m\n",
            "DEBUG flwr 2023-08-22 20:30:03,231 | server.py:182 | evaluate_round 4 received 10 results and 0 failures\n",
            "DEBUG:flwr:evaluate_round 4 received 10 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:30:03,234 | server.py:218 | fit_round 5: strategy sampled 10 clients (out of 100)\n",
            "DEBUG:flwr:fit_round 5: strategy sampled 10 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1953645)\u001b[0m WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_test_function.<locals>.test_function at 0x7a85b0423640> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1953645)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1953643)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1953643)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1953643)\u001b[0m 2/2 - 0s - loss: 3.2403 - accuracy: 0.1000 - 279ms/epoch - 139ms/step\u001b[32m [repeated 9x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1953650)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 7x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1953650)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1953648)\u001b[0m 15/15 - 4s - loss: 2.1345 - accuracy: 0.2444 - 4s/epoch - 234ms/step\u001b[32m [repeated 8x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-08-22 20:30:32,122 | server.py:232 | fit_round 5 received 10 results and 0 failures\n",
            "DEBUG:flwr:fit_round 5 received 10 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:30:32,181 | server.py:168 | evaluate_round 5: strategy sampled 10 clients (out of 100)\n",
            "DEBUG:flwr:evaluate_round 5: strategy sampled 10 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1953649)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 2x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1953648)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1953649)\u001b[0m 15/15 - 1s - loss: 2.2002 - accuracy: 0.2311 - 1s/epoch - 85ms/step\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1953648)\u001b[0m WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_test_function.<locals>.test_function at 0x7fed4c1ee710> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1953648)\u001b[0m WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_test_function.<locals>.test_function at 0x7fed4c1ee710> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1953648)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 8x across cluster]\u001b[0m\n",
            "DEBUG flwr 2023-08-22 20:30:58,998 | server.py:182 | evaluate_round 5 received 10 results and 0 failures\n",
            "DEBUG:flwr:evaluate_round 5 received 10 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:30:59,003 | server.py:218 | fit_round 6: strategy sampled 10 clients (out of 100)\n",
            "DEBUG:flwr:fit_round 6: strategy sampled 10 clients (out of 100)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1953649)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1953649)\u001b[0m 2/2 - 0s - loss: 3.3731 - accuracy: 0.1600 - 319ms/epoch - 159ms/step\u001b[32m [repeated 10x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1953648)\u001b[0m WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7fed4ced3640> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1953643)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 2x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1953643)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1953648)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1953650)\u001b[0m 15/15 - 3s - loss: 2.1637 - accuracy: 0.2644 - 3s/epoch - 197ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1953650)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 8x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1953650)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 7x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-08-22 20:31:29,283 | server.py:232 | fit_round 6 received 10 results and 0 failures\n",
            "DEBUG:flwr:fit_round 6 received 10 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:31:29,346 | server.py:168 | evaluate_round 6: strategy sampled 10 clients (out of 100)\n",
            "DEBUG:flwr:evaluate_round 6: strategy sampled 10 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1953643)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 2x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1953643)\u001b[0m 15/15 - 1s - loss: 2.1098 - accuracy: 0.2667 - 1s/epoch - 98ms/step\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1953650)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1953646)\u001b[0m WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f6f857835b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1953646)\u001b[0m WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f6f857835b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "DEBUG flwr 2023-08-22 20:31:54,757 | server.py:182 | evaluate_round 6 received 10 results and 0 failures\n",
            "DEBUG:flwr:evaluate_round 6 received 10 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:31:54,761 | server.py:218 | fit_round 7: strategy sampled 10 clients (out of 100)\n",
            "DEBUG:flwr:fit_round 7: strategy sampled 10 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1953643)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 10x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1953643)\u001b[0m 2/2 - 0s - loss: 1.9110 - accuracy: 0.4200 - 277ms/epoch - 138ms/step\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1953643)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 10x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1953643)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 8x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1953647)\u001b[0m 15/15 - 3s - loss: 1.9043 - accuracy: 0.3000 - 3s/epoch - 209ms/step\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1953643)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 8x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-08-22 20:32:23,512 | server.py:232 | fit_round 7 received 10 results and 0 failures\n",
            "DEBUG:flwr:fit_round 7 received 10 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:32:23,572 | server.py:168 | evaluate_round 7: strategy sampled 10 clients (out of 100)\n",
            "DEBUG:flwr:evaluate_round 7: strategy sampled 10 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1953650)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 2x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1953650)\u001b[0m 15/15 - 1s - loss: 1.9042 - accuracy: 0.3133 - 1s/epoch - 92ms/step\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1953650)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-08-22 20:32:48,620 | server.py:182 | evaluate_round 7 received 10 results and 0 failures\n",
            "DEBUG:flwr:evaluate_round 7 received 10 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:32:48,624 | server.py:218 | fit_round 8: strategy sampled 10 clients (out of 100)\n",
            "DEBUG:flwr:fit_round 8: strategy sampled 10 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1953643)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 10x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1953643)\u001b[0m 2/2 - 0s - loss: 1.8353 - accuracy: 0.2200 - 283ms/epoch - 141ms/step\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1953643)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 10x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1953643)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 8x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1953646)\u001b[0m 15/15 - 4s - loss: 1.9165 - accuracy: 0.3111 - 4s/epoch - 247ms/step\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1953646)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 7x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-08-22 20:33:18,643 | server.py:232 | fit_round 8 received 10 results and 0 failures\n",
            "DEBUG:flwr:fit_round 8 received 10 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:33:18,706 | server.py:168 | evaluate_round 8: strategy sampled 10 clients (out of 100)\n",
            "DEBUG:flwr:evaluate_round 8: strategy sampled 10 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1953645)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 2x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1953645)\u001b[0m 15/15 - 1s - loss: 1.9091 - accuracy: 0.3044 - 1s/epoch - 94ms/step\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1953645)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-08-22 20:33:44,146 | server.py:182 | evaluate_round 8 received 10 results and 0 failures\n",
            "DEBUG:flwr:evaluate_round 8 received 10 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:33:44,148 | server.py:218 | fit_round 9: strategy sampled 10 clients (out of 100)\n",
            "DEBUG:flwr:fit_round 9: strategy sampled 10 clients (out of 100)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1953645)\u001b[0m 2/2 - 0s - loss: 2.5986 - accuracy: 0.1600 - 294ms/epoch - 147ms/step\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1953645)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 9x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1953643)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 10x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1953643)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1953648)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1953648)\u001b[0m 15/15 - 3s - loss: 1.8663 - accuracy: 0.3200 - 3s/epoch - 218ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1953648)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 8x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1953648)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 7x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-08-22 20:34:13,856 | server.py:232 | fit_round 9 received 10 results and 0 failures\n",
            "DEBUG:flwr:fit_round 9 received 10 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:34:13,915 | server.py:168 | evaluate_round 9: strategy sampled 10 clients (out of 100)\n",
            "DEBUG:flwr:evaluate_round 9: strategy sampled 10 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1953646)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 2x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1953643)\u001b[0m 15/15 - 1s - loss: 1.8542 - accuracy: 0.3400 - 1s/epoch - 90ms/step\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1953646)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-08-22 20:34:39,210 | server.py:182 | evaluate_round 9 received 10 results and 0 failures\n",
            "DEBUG:flwr:evaluate_round 9 received 10 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:34:39,212 | server.py:218 | fit_round 10: strategy sampled 10 clients (out of 100)\n",
            "DEBUG:flwr:fit_round 10: strategy sampled 10 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1953643)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 10x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1953643)\u001b[0m 2/2 - 0s - loss: 1.5713 - accuracy: 0.3600 - 275ms/epoch - 138ms/step\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1953643)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1953650)\u001b[0m 15/15 - 4s - loss: 1.8068 - accuracy: 0.3756 - 4s/epoch - 238ms/step\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1953644)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 7x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1953643)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 8x across cluster]\u001b[0m\n",
            "DEBUG flwr 2023-08-22 20:35:09,321 | server.py:232 | fit_round 10 received 10 results and 0 failures\n",
            "DEBUG:flwr:fit_round 10 received 10 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:35:09,384 | server.py:168 | evaluate_round 10: strategy sampled 10 clients (out of 100)\n",
            "DEBUG:flwr:evaluate_round 10: strategy sampled 10 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1953646)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 2x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1953643)\u001b[0m 15/15 - 1s - loss: 1.8350 - accuracy: 0.3156 - 1s/epoch - 93ms/step\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1953646)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-08-22 20:35:34,599 | server.py:182 | evaluate_round 10 received 10 results and 0 failures\n",
            "DEBUG:flwr:evaluate_round 10 received 10 results and 0 failures\n",
            "INFO flwr 2023-08-22 20:35:34,601 | server.py:147 | FL finished in 609.5048693400004\n",
            "INFO:flwr:FL finished in 609.5048693400004\n",
            "INFO flwr 2023-08-22 20:35:34,606 | app.py:218 | app_fit: losses_distributed [(1, 2.26438045501709), (2, 2.1113085627555845), (3, 2.5446860074996946), (4, 3.6562504529953004), (5, 3.3602102041244506), (6, 1.9830422401428223), (7, 1.9878299593925477), (8, 2.0472349286079408), (9, 1.7630190968513488), (10, 2.4638048887252806)]\n",
            "INFO:flwr:app_fit: losses_distributed [(1, 2.26438045501709), (2, 2.1113085627555845), (3, 2.5446860074996946), (4, 3.6562504529953004), (5, 3.3602102041244506), (6, 1.9830422401428223), (7, 1.9878299593925477), (8, 2.0472349286079408), (9, 1.7630190968513488), (10, 2.4638048887252806)]\n",
            "INFO flwr 2023-08-22 20:35:34,610 | app.py:219 | app_fit: metrics_distributed_fit {}\n",
            "INFO:flwr:app_fit: metrics_distributed_fit {}\n",
            "INFO flwr 2023-08-22 20:35:34,613 | app.py:220 | app_fit: metrics_distributed {}\n",
            "INFO:flwr:app_fit: metrics_distributed {}\n",
            "INFO flwr 2023-08-22 20:35:34,616 | app.py:221 | app_fit: losses_centralized []\n",
            "INFO:flwr:app_fit: losses_centralized []\n",
            "INFO flwr 2023-08-22 20:35:34,619 | app.py:222 | app_fit: metrics_centralized {}\n",
            "INFO:flwr:app_fit: metrics_centralized {}\n",
            "INFO flwr 2023-08-22 20:35:34,646 | app.py:146 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)\n",
            "INFO:flwr:Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1953643)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 9x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            ">>> Running with the following parameters:\n",
            ">>>> Concentration (alpha): 100.0\n",
            ">>>> Reporting Fraction (C): 0.1\n",
            ">>>> Local Epochs (E): 5\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1953643)\u001b[0m 2/2 - 0s - loss: 2.9868 - accuracy: 0.1000 - 306ms/epoch - 153ms/step\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1953643)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 9x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-08-22 20:35:39,623\tINFO worker.py:1621 -- Started a local Ray instance.\n",
            "INFO flwr 2023-08-22 20:35:41,200 | app.py:180 | Flower VCE: Ray initialized with resources: {'memory': 32533416347.0, 'object_store_memory': 16266708172.0, 'CPU': 8.0, 'node:__internal_head__': 1.0, 'node:172.28.0.12': 1.0}\n",
            "INFO:flwr:Flower VCE: Ray initialized with resources: {'memory': 32533416347.0, 'object_store_memory': 16266708172.0, 'CPU': 8.0, 'node:__internal_head__': 1.0, 'node:172.28.0.12': 1.0}\n",
            "INFO flwr 2023-08-22 20:35:41,206 | server.py:86 | Initializing global parameters\n",
            "INFO:flwr:Initializing global parameters\n",
            "INFO flwr 2023-08-22 20:35:41,211 | server.py:269 | Using initial parameters provided by strategy\n",
            "INFO:flwr:Using initial parameters provided by strategy\n",
            "INFO flwr 2023-08-22 20:35:41,215 | server.py:88 | Evaluating initial parameters\n",
            "INFO:flwr:Evaluating initial parameters\n",
            "INFO flwr 2023-08-22 20:35:41,219 | server.py:101 | FL starting\n",
            "INFO:flwr:FL starting\n",
            "DEBUG flwr 2023-08-22 20:35:41,223 | server.py:218 | fit_round 1: strategy sampled 10 clients (out of 100)\n",
            "DEBUG:flwr:fit_round 1: strategy sampled 10 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959224)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959224)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959222)\u001b[0m Epoch 1/5\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959224)\u001b[0m 15/15 - 3s - loss: 2.2948 - accuracy: 0.1156 - 3s/epoch - 208ms/step\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959228)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959224)\u001b[0m Epoch 3/5\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959222)\u001b[0m 15/15 - 3s - loss: 2.1070 - accuracy: 0.2489 - 3s/epoch - 171ms/step\u001b[32m [repeated 18x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959219)\u001b[0m Epoch 5/5\u001b[32m [repeated 20x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959226)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 8x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959226)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959218)\u001b[0m 15/15 - 2s - loss: 1.9500 - accuracy: 0.3356 - 2s/epoch - 110ms/step\u001b[32m [repeated 21x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959226)\u001b[0m Epoch 3/5\u001b[32m [repeated 7x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-08-22 20:36:29,281 | server.py:232 | fit_round 1 received 10 results and 0 failures\n",
            "DEBUG:flwr:fit_round 1 received 10 results and 0 failures\n",
            "WARNING flwr 2023-08-22 20:36:29,354 | fedavgm.py:214 | No fit_metrics_aggregation_fn provided\n",
            "WARNING:flwr:No fit_metrics_aggregation_fn provided\n",
            "DEBUG flwr 2023-08-22 20:36:29,356 | server.py:168 | evaluate_round 1: strategy sampled 10 clients (out of 100)\n",
            "DEBUG:flwr:evaluate_round 1: strategy sampled 10 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1959219)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 2x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1959224)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959224)\u001b[0m 15/15 - 0s - loss: 1.7864 - accuracy: 0.3644 - 315ms/epoch - 21ms/step\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959224)\u001b[0m Epoch 5/5\u001b[32m [repeated 6x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-08-22 20:36:55,480 | server.py:182 | evaluate_round 1 received 10 results and 0 failures\n",
            "DEBUG:flwr:evaluate_round 1 received 10 results and 0 failures\n",
            "WARNING flwr 2023-08-22 20:36:55,484 | fedavg.py:274 | No evaluate_metrics_aggregation_fn provided\n",
            "WARNING:flwr:No evaluate_metrics_aggregation_fn provided\n",
            "DEBUG flwr 2023-08-22 20:36:55,487 | server.py:218 | fit_round 2: strategy sampled 10 clients (out of 100)\n",
            "DEBUG:flwr:fit_round 2: strategy sampled 10 clients (out of 100)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1959224)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1959224)\u001b[0m 2/2 - 0s - loss: 2.1148 - accuracy: 0.3200 - 287ms/epoch - 143ms/step\u001b[32m [repeated 10x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959224)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 10x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959224)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959224)\u001b[0m Epoch 1/5\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959219)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959222)\u001b[0m 15/15 - 3s - loss: 2.0202 - accuracy: 0.2778 - 3s/epoch - 177ms/step\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959226)\u001b[0m Epoch 2/5\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959226)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959227)\u001b[0m 15/15 - 2s - loss: 1.6430 - accuracy: 0.4222 - 2s/epoch - 128ms/step\u001b[32m [repeated 22x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959227)\u001b[0m Epoch 5/5\u001b[32m [repeated 23x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959228)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 8x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959228)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959228)\u001b[0m 15/15 - 1s - loss: 2.0124 - accuracy: 0.2822 - 1s/epoch - 84ms/step\u001b[32m [repeated 18x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959228)\u001b[0m Epoch 4/5\u001b[32m [repeated 8x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-08-22 20:37:48,405 | server.py:232 | fit_round 2 received 10 results and 0 failures\n",
            "DEBUG:flwr:fit_round 2 received 10 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:37:48,474 | server.py:168 | evaluate_round 2: strategy sampled 10 clients (out of 100)\n",
            "DEBUG:flwr:evaluate_round 2: strategy sampled 10 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1959222)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 2x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1959226)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959222)\u001b[0m 15/15 - 1s - loss: 1.3131 - accuracy: 0.5400 - 521ms/epoch - 35ms/step\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959222)\u001b[0m Epoch 5/5\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1959224)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1959217)\u001b[0m 2/2 - 2s - loss: 3.1026 - accuracy: 0.2200 - 2s/epoch - 1s/step\u001b[32m [repeated 6x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1959226)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 8x across cluster]\u001b[0m\n",
            "DEBUG flwr 2023-08-22 20:38:27,751 | server.py:182 | evaluate_round 2 received 10 results and 0 failures\n",
            "DEBUG:flwr:evaluate_round 2 received 10 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:38:27,766 | server.py:218 | fit_round 3: strategy sampled 10 clients (out of 100)\n",
            "DEBUG:flwr:fit_round 3: strategy sampled 10 clients (out of 100)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1959219)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1959219)\u001b[0m 2/2 - 1s - loss: 3.2000 - accuracy: 0.2200 - 822ms/epoch - 411ms/step\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959226)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 2x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959219)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959226)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959219)\u001b[0m Epoch 1/5\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959226)\u001b[0m 15/15 - 3s - loss: 2.2330 - accuracy: 0.3044 - 3s/epoch - 178ms/step\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959228)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959226)\u001b[0m Epoch 3/5\u001b[32m [repeated 17x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959219)\u001b[0m 15/15 - 2s - loss: 1.4393 - accuracy: 0.4533 - 2s/epoch - 156ms/step\u001b[32m [repeated 19x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959224)\u001b[0m Epoch 5/5\u001b[32m [repeated 17x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959226)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 8x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959222)\u001b[0m 15/15 - 2s - loss: 1.1807 - accuracy: 0.5644 - 2s/epoch - 157ms/step\u001b[32m [repeated 17x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959226)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959226)\u001b[0m Epoch 2/5\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959226)\u001b[0m 15/15 - 1s - loss: 1.3853 - accuracy: 0.5378 - 634ms/epoch - 42ms/step\u001b[32m [repeated 12x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-08-22 20:39:17,079 | server.py:232 | fit_round 3 received 10 results and 0 failures\n",
            "DEBUG:flwr:fit_round 3 received 10 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:39:17,141 | server.py:168 | evaluate_round 3: strategy sampled 10 clients (out of 100)\n",
            "DEBUG:flwr:evaluate_round 3: strategy sampled 10 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1959224)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 2x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1959226)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959224)\u001b[0m Epoch 5/5\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959224)\u001b[0m 15/15 - 0s - loss: 1.4214 - accuracy: 0.4978 - 429ms/epoch - 29ms/step\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1959226)\u001b[0m 2/2 - 1s - loss: 6.2482 - accuracy: 0.3400 - 633ms/epoch - 316ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1959219)\u001b[0m WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_test_function.<locals>.test_function at 0x7e02b828f490> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1959219)\u001b[0m WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_test_function.<locals>.test_function at 0x7e02b828f490> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1959227)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 9x across cluster]\u001b[0m\n",
            "DEBUG flwr 2023-08-22 20:39:42,781 | server.py:182 | evaluate_round 3 received 10 results and 0 failures\n",
            "DEBUG:flwr:evaluate_round 3 received 10 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:39:42,784 | server.py:218 | fit_round 4: strategy sampled 10 clients (out of 100)\n",
            "DEBUG:flwr:fit_round 4: strategy sampled 10 clients (out of 100)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1959227)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1959227)\u001b[0m 2/2 - 0s - loss: 3.6622 - accuracy: 0.4200 - 274ms/epoch - 137ms/step\u001b[32m [repeated 9x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1959226)\u001b[0m WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_test_function.<locals>.test_function at 0x7acabac17400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959226)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959227)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959226)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959226)\u001b[0m Epoch 1/5\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959217)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959226)\u001b[0m 15/15 - 3s - loss: 3.0372 - accuracy: 0.3600 - 3s/epoch - 189ms/step\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959219)\u001b[0m Epoch 3/5\u001b[32m [repeated 17x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959228)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959228)\u001b[0m 15/15 - 2s - loss: 1.4130 - accuracy: 0.5000 - 2s/epoch - 141ms/step\u001b[32m [repeated 21x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959228)\u001b[0m Epoch 5/5\u001b[32m [repeated 20x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959226)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 7x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959226)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959224)\u001b[0m 15/15 - 1s - loss: 1.2039 - accuracy: 0.5822 - 1s/epoch - 83ms/step\u001b[32m [repeated 18x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959226)\u001b[0m Epoch 4/5\u001b[32m [repeated 9x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-08-22 20:40:20,976 | server.py:232 | fit_round 4 received 10 results and 0 failures\n",
            "DEBUG:flwr:fit_round 4 received 10 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:40:21,037 | server.py:168 | evaluate_round 4: strategy sampled 10 clients (out of 100)\n",
            "DEBUG:flwr:evaluate_round 4: strategy sampled 10 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1959219)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 2x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1959219)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959217)\u001b[0m 15/15 - 0s - loss: 1.2069 - accuracy: 0.5911 - 403ms/epoch - 27ms/step\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959217)\u001b[0m Epoch 5/5\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1959219)\u001b[0m WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7e02b828f880> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1959219)\u001b[0m WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7e02b828f880> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "DEBUG flwr 2023-08-22 20:40:45,685 | server.py:182 | evaluate_round 4 received 10 results and 0 failures\n",
            "DEBUG:flwr:evaluate_round 4 received 10 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:40:45,689 | server.py:218 | fit_round 5: strategy sampled 10 clients (out of 100)\n",
            "DEBUG:flwr:fit_round 5: strategy sampled 10 clients (out of 100)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1959222)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\u001b[32m [repeated 9x across cluster]\u001b[0m"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1959222)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 9x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1959222)\u001b[0m WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_test_function.<locals>.test_function at 0x7d4c04183130> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959219)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959222)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1959222)\u001b[0m 2/2 - 0s - loss: 2.2743 - accuracy: 0.4400 - 285ms/epoch - 143ms/step\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959222)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959222)\u001b[0m Epoch 1/5\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959219)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959222)\u001b[0m 15/15 - 2s - loss: 2.0178 - accuracy: 0.4178 - 2s/epoch - 162ms/step\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959218)\u001b[0m Epoch 2/5\u001b[32m [repeated 17x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959218)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959218)\u001b[0m 15/15 - 2s - loss: 1.5728 - accuracy: 0.4511 - 2s/epoch - 153ms/step\u001b[32m [repeated 17x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959227)\u001b[0m Epoch 5/5\u001b[32m [repeated 18x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959222)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 7x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959218)\u001b[0m 15/15 - 1s - loss: 1.0221 - accuracy: 0.6444 - 1s/epoch - 67ms/step\u001b[32m [repeated 22x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959222)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959222)\u001b[0m Epoch 3/5\u001b[32m [repeated 9x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-08-22 20:41:24,294 | server.py:232 | fit_round 5 received 10 results and 0 failures\n",
            "DEBUG:flwr:fit_round 5 received 10 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:41:24,356 | server.py:168 | evaluate_round 5: strategy sampled 10 clients (out of 100)\n",
            "DEBUG:flwr:evaluate_round 5: strategy sampled 10 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1959226)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 2x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959219)\u001b[0m 15/15 - 1s - loss: 0.9223 - accuracy: 0.6844 - 529ms/epoch - 35ms/step\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1959218)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959219)\u001b[0m Epoch 5/5\u001b[32m [repeated 5x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1959218)\u001b[0m WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_test_function.<locals>.test_function at 0x7d53a83cbb50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1959218)\u001b[0m WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_test_function.<locals>.test_function at 0x7d53a83cbb50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1959218)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 9x across cluster]\u001b[0m\n",
            "DEBUG flwr 2023-08-22 20:41:50,927 | server.py:182 | evaluate_round 5 received 10 results and 0 failures\n",
            "DEBUG:flwr:evaluate_round 5 received 10 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:41:50,937 | server.py:218 | fit_round 6: strategy sampled 10 clients (out of 100)\n",
            "DEBUG:flwr:fit_round 6: strategy sampled 10 clients (out of 100)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1959226)\u001b[0m 2/2 - 1s - loss: 2.3338 - accuracy: 0.4400 - 777ms/epoch - 388ms/step\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1959226)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\u001b[32m [repeated 9x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1959218)\u001b[0m WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7d53d9b7bb50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959226)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959218)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959218)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959218)\u001b[0m Epoch 1/5\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959217)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959218)\u001b[0m 15/15 - 3s - loss: 1.8121 - accuracy: 0.4178 - 3s/epoch - 179ms/step\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959222)\u001b[0m Epoch 2/5\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959222)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959227)\u001b[0m 15/15 - 2s - loss: 1.0056 - accuracy: 0.6533 - 2s/epoch - 134ms/step\u001b[32m [repeated 18x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959226)\u001b[0m Epoch 5/5\u001b[32m [repeated 18x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959218)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 7x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959218)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959222)\u001b[0m 15/15 - 1s - loss: 0.7810 - accuracy: 0.7333 - 1s/epoch - 84ms/step\u001b[32m [repeated 21x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959218)\u001b[0m Epoch 4/5\u001b[32m [repeated 10x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-08-22 20:42:29,673 | server.py:232 | fit_round 6 received 10 results and 0 failures\n",
            "DEBUG:flwr:fit_round 6 received 10 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:42:29,736 | server.py:168 | evaluate_round 6: strategy sampled 10 clients (out of 100)\n",
            "DEBUG:flwr:evaluate_round 6: strategy sampled 10 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1959217)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 2x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1959217)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959217)\u001b[0m 15/15 - 0s - loss: 0.8212 - accuracy: 0.7444 - 335ms/epoch - 22ms/step\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959217)\u001b[0m Epoch 5/5\u001b[32m [repeated 5x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1959217)\u001b[0m WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x787ae308a320> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1959217)\u001b[0m WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x787ae308a320> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "DEBUG flwr 2023-08-22 20:42:54,651 | server.py:182 | evaluate_round 6 received 10 results and 0 failures\n",
            "DEBUG:flwr:evaluate_round 6 received 10 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:42:54,654 | server.py:218 | fit_round 7: strategy sampled 10 clients (out of 100)\n",
            "DEBUG:flwr:fit_round 7: strategy sampled 10 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959222)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 10x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1959228)\u001b[0m WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7be5b41e3370> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959222)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1959222)\u001b[0m 2/2 - 0s - loss: 1.9574 - accuracy: 0.3400 - 270ms/epoch - 135ms/step\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959222)\u001b[0m Epoch 1/5\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959219)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959227)\u001b[0m 15/15 - 2s - loss: 1.2998 - accuracy: 0.5400 - 2s/epoch - 126ms/step\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959227)\u001b[0m Epoch 3/5\u001b[32m [repeated 17x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959218)\u001b[0m 15/15 - 2s - loss: 0.8899 - accuracy: 0.6889 - 2s/epoch - 143ms/step\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959218)\u001b[0m Epoch 5/5\u001b[32m [repeated 20x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959222)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 8x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959222)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959227)\u001b[0m 15/15 - 0s - loss: 1.3174 - accuracy: 0.5644 - 496ms/epoch - 33ms/step\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959227)\u001b[0m Epoch 3/5\u001b[32m [repeated 9x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-08-22 20:43:33,323 | server.py:232 | fit_round 7 received 10 results and 0 failures\n",
            "DEBUG:flwr:fit_round 7 received 10 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:43:33,384 | server.py:168 | evaluate_round 7: strategy sampled 10 clients (out of 100)\n",
            "DEBUG:flwr:evaluate_round 7: strategy sampled 10 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1959227)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 2x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1959227)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959227)\u001b[0m 15/15 - 0s - loss: 0.6391 - accuracy: 0.8222 - 319ms/epoch - 21ms/step\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959227)\u001b[0m Epoch 5/5\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-08-22 20:43:57,718 | server.py:182 | evaluate_round 7 received 10 results and 0 failures\n",
            "DEBUG:flwr:evaluate_round 7 received 10 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:43:57,720 | server.py:218 | fit_round 8: strategy sampled 10 clients (out of 100)\n",
            "DEBUG:flwr:fit_round 8: strategy sampled 10 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959224)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 10x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959224)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1959224)\u001b[0m 2/2 - 0s - loss: 1.9449 - accuracy: 0.4200 - 284ms/epoch - 142ms/step\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959224)\u001b[0m Epoch 1/5\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959222)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959224)\u001b[0m 15/15 - 2s - loss: 1.1784 - accuracy: 0.5911 - 2s/epoch - 148ms/step\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959224)\u001b[0m Epoch 3/5\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959226)\u001b[0m 15/15 - 2s - loss: 0.6166 - accuracy: 0.8044 - 2s/epoch - 119ms/step\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959228)\u001b[0m Epoch 5/5\u001b[32m [repeated 19x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959224)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 8x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959224)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959224)\u001b[0m 15/15 - 1s - loss: 0.7512 - accuracy: 0.7444 - 547ms/epoch - 36ms/step\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959224)\u001b[0m Epoch 4/5\u001b[32m [repeated 11x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-08-22 20:44:35,737 | server.py:232 | fit_round 8 received 10 results and 0 failures\n",
            "DEBUG:flwr:fit_round 8 received 10 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:44:35,797 | server.py:168 | evaluate_round 8: strategy sampled 10 clients (out of 100)\n",
            "DEBUG:flwr:evaluate_round 8: strategy sampled 10 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1959227)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 2x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1959227)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959227)\u001b[0m 15/15 - 0s - loss: 0.4901 - accuracy: 0.8511 - 399ms/epoch - 27ms/step\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959227)\u001b[0m Epoch 5/5\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-08-22 20:45:00,389 | server.py:182 | evaluate_round 8 received 10 results and 0 failures\n",
            "DEBUG:flwr:evaluate_round 8 received 10 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:45:00,395 | server.py:218 | fit_round 9: strategy sampled 10 clients (out of 100)\n",
            "DEBUG:flwr:fit_round 9: strategy sampled 10 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959224)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 10x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959224)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1959224)\u001b[0m 2/2 - 0s - loss: 2.0166 - accuracy: 0.5200 - 289ms/epoch - 144ms/step\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959224)\u001b[0m Epoch 1/5\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959226)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959227)\u001b[0m 15/15 - 2s - loss: 1.1782 - accuracy: 0.5778 - 2s/epoch - 137ms/step\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959227)\u001b[0m Epoch 3/5\u001b[32m [repeated 17x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959226)\u001b[0m 15/15 - 3s - loss: 0.5927 - accuracy: 0.8178 - 3s/epoch - 171ms/step\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959217)\u001b[0m Epoch 5/5\u001b[32m [repeated 19x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959224)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 8x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959224)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959224)\u001b[0m 15/15 - 1s - loss: 1.6654 - accuracy: 0.4911 - 1s/epoch - 99ms/step\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959224)\u001b[0m Epoch 2/5\u001b[32m [repeated 6x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-08-22 20:45:44,610 | server.py:232 | fit_round 9 received 10 results and 0 failures\n",
            "DEBUG:flwr:fit_round 9 received 10 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:45:44,676 | server.py:168 | evaluate_round 9: strategy sampled 10 clients (out of 100)\n",
            "DEBUG:flwr:evaluate_round 9: strategy sampled 10 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1959217)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 2x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1959217)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959227)\u001b[0m 15/15 - 0s - loss: 0.2973 - accuracy: 0.9244 - 450ms/epoch - 30ms/step\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959227)\u001b[0m Epoch 5/5\u001b[32m [repeated 7x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-08-22 20:46:09,148 | server.py:182 | evaluate_round 9 received 10 results and 0 failures\n",
            "DEBUG:flwr:evaluate_round 9 received 10 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:46:09,150 | server.py:218 | fit_round 10: strategy sampled 10 clients (out of 100)\n",
            "DEBUG:flwr:fit_round 10: strategy sampled 10 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959227)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 10x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959227)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1959227)\u001b[0m 2/2 - 0s - loss: 2.5399 - accuracy: 0.4000 - 271ms/epoch - 136ms/step\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959227)\u001b[0m Epoch 1/5\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959219)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959227)\u001b[0m 15/15 - 2s - loss: 1.1854 - accuracy: 0.6044 - 2s/epoch - 133ms/step\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959227)\u001b[0m Epoch 3/5\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959222)\u001b[0m 15/15 - 2s - loss: 0.4526 - accuracy: 0.8667 - 2s/epoch - 124ms/step\u001b[32m [repeated 22x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959222)\u001b[0m Epoch 5/5\u001b[32m [repeated 22x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959217)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 8x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959217)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959227)\u001b[0m 15/15 - 2s - loss: 2.0016 - accuracy: 0.4889 - 2s/epoch - 106ms/step\u001b[32m [repeated 13x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959227)\u001b[0m Epoch 2/5\u001b[32m [repeated 7x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-08-22 20:46:47,602 | server.py:232 | fit_round 10 received 10 results and 0 failures\n",
            "DEBUG:flwr:fit_round 10 received 10 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:46:47,664 | server.py:168 | evaluate_round 10: strategy sampled 10 clients (out of 100)\n",
            "DEBUG:flwr:evaluate_round 10: strategy sampled 10 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1959219)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 2x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1959222)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959227)\u001b[0m 15/15 - 0s - loss: 0.3238 - accuracy: 0.9156 - 296ms/epoch - 20ms/step\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1959227)\u001b[0m Epoch 5/5\u001b[32m [repeated 5x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-08-22 20:47:12,621 | server.py:182 | evaluate_round 10 received 10 results and 0 failures\n",
            "DEBUG:flwr:evaluate_round 10 received 10 results and 0 failures\n",
            "INFO flwr 2023-08-22 20:47:12,626 | server.py:147 | FL finished in 691.4034725889996\n",
            "INFO:flwr:FL finished in 691.4034725889996\n",
            "INFO flwr 2023-08-22 20:47:12,629 | app.py:218 | app_fit: losses_distributed [(1, 2.131155276298523), (2, 3.362056279182434), (3, 5.77343213558197), (4, 2.418073761463165), (5, 2.0140365958213806), (6, 2.109922635555267), (7, 1.9499183893203735), (8, 2.0344620823860167), (9, 2.260772931575775), (10, 1.9852226734161378)]\n",
            "INFO:flwr:app_fit: losses_distributed [(1, 2.131155276298523), (2, 3.362056279182434), (3, 5.77343213558197), (4, 2.418073761463165), (5, 2.0140365958213806), (6, 2.109922635555267), (7, 1.9499183893203735), (8, 2.0344620823860167), (9, 2.260772931575775), (10, 1.9852226734161378)]\n",
            "INFO flwr 2023-08-22 20:47:12,632 | app.py:219 | app_fit: metrics_distributed_fit {}\n",
            "INFO:flwr:app_fit: metrics_distributed_fit {}\n",
            "INFO flwr 2023-08-22 20:47:12,633 | app.py:220 | app_fit: metrics_distributed {}\n",
            "INFO:flwr:app_fit: metrics_distributed {}\n",
            "INFO flwr 2023-08-22 20:47:12,636 | app.py:221 | app_fit: losses_centralized []\n",
            "INFO:flwr:app_fit: losses_centralized []\n",
            "INFO flwr 2023-08-22 20:47:12,638 | app.py:222 | app_fit: metrics_centralized {}\n",
            "INFO:flwr:app_fit: metrics_centralized {}\n",
            "INFO flwr 2023-08-22 20:47:12,661 | app.py:146 | Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)\n",
            "INFO:flwr:Starting Flower simulation, config: ServerConfig(num_rounds=10, round_timeout=None)\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1959219)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 9x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            ">>> Running with the following parameters:\n",
            ">>>> Concentration (alpha): 100.0\n",
            ">>>> Reporting Fraction (C): 0.4\n",
            ">>>> Local Epochs (E): 1\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1959219)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 5\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1959222)\u001b[0m 2/2 - 0s - loss: 2.1267 - accuracy: 0.5000 - 273ms/epoch - 137ms/step\u001b[32m [repeated 9x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-08-22 20:47:17,460\tINFO worker.py:1621 -- Started a local Ray instance.\n",
            "INFO flwr 2023-08-22 20:47:19,092 | app.py:180 | Flower VCE: Ray initialized with resources: {'object_store_memory': 16263026688.0, 'memory': 32526053376.0, 'CPU': 8.0, 'node:__internal_head__': 1.0, 'node:172.28.0.12': 1.0}\n",
            "INFO:flwr:Flower VCE: Ray initialized with resources: {'object_store_memory': 16263026688.0, 'memory': 32526053376.0, 'CPU': 8.0, 'node:__internal_head__': 1.0, 'node:172.28.0.12': 1.0}\n",
            "INFO flwr 2023-08-22 20:47:19,101 | server.py:86 | Initializing global parameters\n",
            "INFO:flwr:Initializing global parameters\n",
            "INFO flwr 2023-08-22 20:47:19,106 | server.py:269 | Using initial parameters provided by strategy\n",
            "INFO:flwr:Using initial parameters provided by strategy\n",
            "INFO flwr 2023-08-22 20:47:19,109 | server.py:88 | Evaluating initial parameters\n",
            "INFO:flwr:Evaluating initial parameters\n",
            "INFO flwr 2023-08-22 20:47:19,113 | server.py:101 | FL starting\n",
            "INFO:flwr:FL starting\n",
            "DEBUG flwr 2023-08-22 20:47:19,115 | server.py:218 | fit_round 1: strategy sampled 40 clients (out of 100)\n",
            "DEBUG:flwr:fit_round 1: strategy sampled 40 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1965082)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1965082)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1965082)\u001b[0m 15/15 - 3s - loss: 2.3061 - accuracy: 0.0822 - 3s/epoch - 226ms/step\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1965078)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1965081)\u001b[0m 15/15 - 3s - loss: 2.3135 - accuracy: 0.0933 - 3s/epoch - 227ms/step\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1965075)\u001b[0m 15/15 - 4s - loss: 2.2897 - accuracy: 0.1333 - 4s/epoch - 236ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1965082)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 8x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1965075)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1965080)\u001b[0m 15/15 - 4s - loss: 2.3001 - accuracy: 0.1800 - 4s/epoch - 265ms/step\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1965077)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1965079)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 4x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1965082)\u001b[0m 15/15 - 2s - loss: 2.3097 - accuracy: 0.0889 - 2s/epoch - 149ms/step\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1965078)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1965082)\u001b[0m 15/15 - 4s - loss: 2.2981 - accuracy: 0.1356 - 4s/epoch - 254ms/step\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1965081)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1965075)\u001b[0m 15/15 - 4s - loss: 2.2988 - accuracy: 0.1000 - 4s/epoch - 290ms/step\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1965078)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 8x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1965078)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1965078)\u001b[0m 15/15 - 9s - loss: 2.3102 - accuracy: 0.1467 - 9s/epoch - 572ms/step\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1965076)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1965079)\u001b[0m 15/15 - 7s - loss: 2.2974 - accuracy: 0.1711 - 7s/epoch - 490ms/step\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1965076)\u001b[0m 15/15 - 6s - loss: 2.3089 - accuracy: 0.1044 - 6s/epoch - 392ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(raylet)\u001b[0m Spilled 2364 MiB, 6 objects, write throughput 85 MiB/s. Set RAY_verbose_spill_logs=0 to disable this message.\n",
            "\u001b[2m\u001b[36m(raylet)\u001b[0m Spilled 4721 MiB, 10 objects, write throughput 139 MiB/s.\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1965076)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 4x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1965076)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1965076)\u001b[0m 15/15 - 3s - loss: 2.3150 - accuracy: 0.1422 - 3s/epoch - 229ms/step\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1965081)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1965081)\u001b[0m 15/15 - 4s - loss: 2.3114 - accuracy: 0.1111 - 4s/epoch - 250ms/step\u001b[32m [repeated 7x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1965082)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 8x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1965075)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 4x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1965075)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1965075)\u001b[0m 15/15 - 2s - loss: 2.2990 - accuracy: 0.1133 - 2s/epoch - 146ms/step\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG flwr 2023-08-22 20:49:38,130 | server.py:232 | fit_round 1 received 40 results and 0 failures\n",
            "DEBUG:flwr:fit_round 1 received 40 results and 0 failures\n",
            "WARNING flwr 2023-08-22 20:49:38,468 | fedavgm.py:214 | No fit_metrics_aggregation_fn provided\n",
            "WARNING:flwr:No fit_metrics_aggregation_fn provided\n",
            "DEBUG flwr 2023-08-22 20:49:38,471 | server.py:168 | evaluate_round 1: strategy sampled 40 clients (out of 100)\n",
            "DEBUG:flwr:evaluate_round 1: strategy sampled 40 clients (out of 100)\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1965082)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 4x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1965082)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1965082)\u001b[0m 15/15 - 2s - loss: 2.2939 - accuracy: 0.1489 - 2s/epoch - 124ms/step\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1965080)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1965082)\u001b[0m 2/2 - 1s - loss: 2.2964 - accuracy: 0.0800 - 607ms/epoch - 303ms/step\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1965076)\u001b[0m 2/2 - 1s - loss: 2.2855 - accuracy: 0.2200 - 582ms/epoch - 291ms/step\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1965078)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1965082)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 8x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1965082)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1965078)\u001b[0m 2/2 - 1s - loss: 2.2915 - accuracy: 0.1600 - 527ms/epoch - 263ms/step\u001b[32m [repeated 6x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1965075)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 7x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1965079)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1965079)\u001b[0m 2/2 - 0s - loss: 2.2932 - accuracy: 0.2200 - 289ms/epoch - 144ms/step\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1965077)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 5x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1965077)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1965081)\u001b[0m 2/2 - 1s - loss: 2.2894 - accuracy: 0.2200 - 548ms/epoch - 274ms/step\u001b[32m [repeated 8x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(raylet)\u001b[0m Spilled 8257 MiB, 16 objects, write throughput 195 MiB/s.\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1965082)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 4x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1965076)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1965076)\u001b[0m 2/2 - 0s - loss: 2.2887 - accuracy: 0.2200 - 390ms/epoch - 195ms/step\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1965082)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1965079)\u001b[0m 2/2 - 0s - loss: 2.2884 - accuracy: 0.2200 - 283ms/epoch - 142ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1965082)\u001b[0m WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_test_function.<locals>.test_function at 0x7dc4ac5f0430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1965082)\u001b[0m WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_test_function.<locals>.test_function at 0x7dc4ac5f0430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1965078)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 7x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1965082)\u001b[0m 2/2 - 1s - loss: 2.2895 - accuracy: 0.1200 - 506ms/epoch - 253ms/step\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1965078)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1965075)\u001b[0m 2/2 - 1s - loss: 2.2958 - accuracy: 0.1400 - 691ms/epoch - 346ms/step\u001b[32m [repeated 7x across cluster]\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1965082)\u001b[0m WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7dc4ac5f35b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1965077)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "DEBUG flwr 2023-08-22 20:51:10,564 | server.py:182 | evaluate_round 1 received 40 results and 0 failures\n",
            "DEBUG:flwr:evaluate_round 1 received 40 results and 0 failures\n",
            "WARNING flwr 2023-08-22 20:51:10,567 | fedavg.py:274 | No evaluate_metrics_aggregation_fn provided\n",
            "WARNING:flwr:No evaluate_metrics_aggregation_fn provided\n",
            "DEBUG flwr 2023-08-22 20:51:10,571 | server.py:218 | fit_round 2: strategy sampled 40 clients (out of 100)\n",
            "DEBUG:flwr:fit_round 2: strategy sampled 40 clients (out of 100)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1965079)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 8x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1965077)\u001b[0m WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7dea962e3a30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1965082)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 5x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_evaluate pid=1965077)\u001b[0m 2/2 - 0s - loss: 2.2900 - accuracy: 0.1400 - 300ms/epoch - 150ms/step\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1965082)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1965081)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1965078)\u001b[0m 15/15 - 3s - loss: 2.2011 - accuracy: 0.2111 - 3s/epoch - 211ms/step\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1965079)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 6x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1965082)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 8x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1965076)\u001b[0m 15/15 - 4s - loss: 2.1737 - accuracy: 0.1711 - 4s/epoch - 283ms/step\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1965082)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1965078)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1965080)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1965078)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 2x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1965078)\u001b[0m 15/15 - 2s - loss: 2.1996 - accuracy: 0.1600 - 2s/epoch - 159ms/step\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1965078)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1965078)\u001b[0m 15/15 - 3s - loss: 2.1841 - accuracy: 0.1956 - 3s/epoch - 184ms/step\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1965077)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1965082)\u001b[0m 15/15 - 3s - loss: 2.2322 - accuracy: 0.1556 - 3s/epoch - 195ms/step\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1965075)\u001b[0m 15/15 - 3s - loss: 2.1985 - accuracy: 0.1844 - 3s/epoch - 215ms/step\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1965076)\u001b[0m 15/15 - 3s - loss: 2.2392 - accuracy: 0.1400 - 3s/epoch - 227ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1965082)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 8x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1965075)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1965077)\u001b[0m 15/15 - 3s - loss: 2.2153 - accuracy: 0.1622 - 3s/epoch - 228ms/step\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1965076)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 4x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1965076)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1965082)\u001b[0m 15/15 - 2s - loss: 2.2124 - accuracy: 0.1311 - 2s/epoch - 158ms/step\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1965079)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1965076)\u001b[0m 15/15 - 3s - loss: 2.2099 - accuracy: 0.1800 - 3s/epoch - 217ms/step\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1965078)\u001b[0m WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 17 variables whereas the saved optimizer has 1 variables. \u001b[32m [repeated 8x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(launch_and_fit pid=1965076)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1965082)\u001b[0m 15/15 - 4s - loss: 2.2342 - accuracy: 0.1511 - 4s/epoch - 260ms/step\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "\u001b[2m\u001b[36m(launch_and_fit pid=1965078)\u001b[0m [!!!!!!!!!!] CONCENTRATION: 100.0 | LOCAL_EPOCHS: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG flwr 2023-08-22 20:52:55,552 | server.py:232 | fit_round 2 received 40 results and 0 failures\n",
            "DEBUG:flwr:fit_round 2 received 40 results and 0 failures\n",
            "DEBUG flwr 2023-08-22 20:52:55,762 | server.py:168 | evaluate_round 2: strategy sampled 40 clients (out of 100)\n",
            "DEBUG:flwr:evaluate_round 2: strategy sampled 40 clients (out of 100)\n"
          ]
        }
      ],
      "source": [
        "# To reproduce Figure 5 from the Paper\n",
        "from collections import defaultdict\n",
        "\n",
        "fedavgm = defaultdict(list)\n",
        "\n",
        "for CONCENTRATION in [100., 10., 1., 0.5, 0.2, 0.1, 0.05, 0.0]:\n",
        "  for REPORTING_FRACTION in [0.05, 0.1, 0.4]:\n",
        "    for LOCAL_EPOCHS in [1, 5]:\n",
        "      print(f\"\\n\\n>>> Running with the following parameters:\\n>>>> Concentration (alpha): {CONCENTRATION}\\n>>>> Reporting Fraction (C): {REPORTING_FRACTION}\\n>>>> Local Epochs (E): {LOCAL_EPOCHS}\\n\\n\")\n",
        "      # Create FedAvgM strategy\n",
        "      strategy_fedavgm = fl.server.strategy.FedAvgM(\n",
        "          fraction_fit=REPORTING_FRACTION,\n",
        "          fraction_evaluate=REPORTING_FRACTION,\n",
        "          server_learning_rate=LEARNING_RATE,\n",
        "          server_momentum=MOMENTUM,\n",
        "          initial_parameters = fl.common.ndarrays_to_parameters(model.get_weights())\n",
        "      )\n",
        "\n",
        "      # Start simulation\n",
        "      hist = fl.simulation.start_simulation(\n",
        "          client_fn=client_fn,\n",
        "          num_clients=NUM_CLIENTS,\n",
        "          config=fl.server.ServerConfig(num_rounds=10), #COMMUNICATION_ROUNDS),\n",
        "          strategy=strategy_fedavgm,\n",
        "      )\n",
        "      # TODO: Calculate Accuracy on the test set\n",
        "      fedavgm[(CONCENTRATION, REPORTING_FRACTION, LOCAL_EPOCHS)].append(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FedAvg Simulation"
      ],
      "metadata": {
        "id": "zzt6zcH44deW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fedavg = defaultdict(list)\n",
        "\n",
        "for CONCENTRATION in [100., 10., 1., 0.5, 0.2, 0.1, 0.05, 0.0]:\n",
        "  for REPORTING_FRACTION in [0.05, 0.1, 0.4]:\n",
        "    for LOCAL_EPOCHS in [1, 5]:\n",
        "      print(f\"\\n\\n>>> Running with the following parameters:\\n>>>> Concentration (alpha): {CONCENTRATION}\\n>>>> Reporting Fraction (C): {REPORTING_FRACTION}\\n>>>> Local Epochs (E): {LOCAL_EPOCHS}\\n\\n\")\n",
        "      # Create FedAvg strategy\n",
        "      strategy_fedavg = fl.server.strategy.FedAvg(\n",
        "          fraction_fit=REPORTING_FRACTION,\n",
        "          fraction_evaluate=REPORTING_FRACTION\n",
        "      )\n",
        "\n",
        "      # Start simulation\n",
        "      hist = fl.simulation.start_simulation(\n",
        "          client_fn=client_fn,\n",
        "          num_clients=NUM_CLIENTS,\n",
        "          config=fl.server.ServerConfig(num_rounds=10), #COMMUNICATION_ROUNDS),\n",
        "          strategy=strategy_fedavg,\n",
        "      )\n",
        "\n",
        "\n",
        "      # TODO: Calculate Accuracy on the test set\n",
        "      fedavg[(CONCENTRATION, REPORTING_FRACTION, LOCAL_EPOCHS)].append(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "FPEmzHVi0kMO",
        "outputId": "a054771c-6400-4779-e827-86ebb6ff9f92"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-375ab7d921c8>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfedavg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mCONCENTRATION\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m100.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mREPORTING_FRACTION\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mLOCAL_EPOCHS\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'defaultdict' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## FedAvgM and FedAvg performance curves for different non-identical-ness"
      ],
      "metadata": {
        "id": "RpTabGAz7MOZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TBD"
      ],
      "metadata": {
        "id": "k0fUb4nf2bdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mlGMxN8w7V-8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyPvyN5iiWLLHS5bga7Ag1nG",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}